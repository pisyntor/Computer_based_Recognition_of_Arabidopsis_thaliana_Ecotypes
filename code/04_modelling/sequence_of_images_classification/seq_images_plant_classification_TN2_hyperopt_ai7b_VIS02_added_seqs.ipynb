{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMePCPau-YjJ"
   },
   "source": [
    "## Imports\n",
    "\n",
    "- importing all torch sub packages. torchvision will be useful to load some CNN architectures and pretrained weights (generally trained on ImageNet)\n",
    "- SciPy for stats\n",
    "- maplotlib for ploting curves and images\n",
    "- tqdm for ploting progress bars\n",
    "- torchmetrics helps to compute the confusion matrix\n",
    "- seaborn for ploting the confusion matrix\n",
    "- pickle for saving/loading stat files (loss, acc, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:23.739437300Z",
     "start_time": "2023-05-28T09:44:23.721360669Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3263,
     "status": "ok",
     "timestamp": 1671181158309,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "3H6-g17F-KyY",
    "outputId": "0808cd1e-0e0f-4c20-bfe7-72707c056e15"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for evaluation \n",
    "from torchmetrics import ConfusionMatrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import shutil\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "default_matplotlib_backend = matplotlib.get_backend()\n",
    "print('imported')\n",
    "print('default_matplotlib_backend: {}'.format(default_matplotlib_backend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-RuHNja-aMb"
   },
   "source": [
    "## Constants\n",
    "What you should know :\n",
    "- Fixing the seed helps to get the same behavious and avoid anomalies when changing operating systems ...\n",
    "- Weights and stats will be saved on their own folders inside the root folder\n",
    "- npy data are used to load quicly the images into memory (reading files from drive takes too much time and slows training)\n",
    "- fixing other constants like \"backbone\" and \"optimizer\", this will decide which architecture and which optimizer will be user for the training so modify only those constants and nothing else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:40.162742553Z",
     "start_time": "2023-05-28T09:44:40.155754368Z"
    }
   },
   "outputs": [],
   "source": [
    "RUN_MODE = ['DEV','LIVE'][1]\n",
    "\n",
    "SIMPLE_PATH = False  # Set this to false if you want to use custom paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:42.334739654Z",
     "start_time": "2023-05-28T09:44:42.265076407Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1671181158674,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "N0skx1mP-K2T",
    "outputId": "0b25acf7-b061-4b1e-c9a9-92fbf1264d45"
   },
   "outputs": [],
   "source": [
    "# Ensure same data distribution between machines\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if running on Windows\n",
    "windows = (os.name == 'nt')\n",
    "\n",
    "# Define paths to data and directories\n",
    "def create_folder(new_path):\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "if SIMPLE_PATH or windows:\n",
    "    # Paths for Windows or using root paths\n",
    "    root_path = os.getcwd()\n",
    "    extracted_data_path = os.path.join(root_path, \"datasets\", \"dataset_4a_n_crop\")\n",
    "    weights_path = os.path.join(root_path, \"weights\")\n",
    "    stats_path = os.path.join(root_path, \"stats\")\n",
    "    npy_data_path = os.path.join(root_path, \"npy_data\")\n",
    "    model_save_path = os.path.join(root_path, \"model\")\n",
    "    pretrained_models_folder = os.path.join(root_path, \"pretrained_models\")\n",
    "else:\n",
    "    # Paths for Linux or custom paths\n",
    "    files_path_name = \"\"\n",
    "    root_path = \"\"\n",
    "    extracted_data_path = \"\"\n",
    "    weights_path =  \"\"\n",
    "    stats_path = \"\"\n",
    "    npy_data_path = \"\"\n",
    "    model_save_path = \"\"\n",
    "    pretrained_models_folder = \"\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in [npy_data_path, weights_path, stats_path, model_save_path, pretrained_models_folder]:\n",
    "    create_folder(path)\n",
    "\n",
    "# Get list of classes\n",
    "class_list = os.listdir(extracted_data_path)\n",
    "class_list.sort()\n",
    "print('Number of classes: {}'.format(len(class_list)))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Selected device: {}'.format(device))\n",
    "if device.type != 'cpu':\n",
    "    print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# Other constants\n",
    "BATCH_SIZE = 8 if RUN_MODE == 'DEV' else 16\n",
    "DAY_TO_USE = 8\n",
    "IMAGES_IN_A_DAY = 2\n",
    "SEQ_LEN = DAY_TO_USE * IMAGES_IN_A_DAY\n",
    "EPOCHS = 60\n",
    "IMG_SIZE = (350, 350)\n",
    "RESIZE_SIZE = (256, 256)\n",
    "NUM_CHANNELS = 3\n",
    "BACKBONE = 'EfficientNetB2'\n",
    "OPTIMIZER = 'Adam'\n",
    "\n",
    "available_backbones = [\n",
    "    'AlexNet', 'ResNet18', 'ResNet34', 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2',\n",
    "    'EfficientNetV2_S', 'ConvNext_T', 'MobileNet_V3_Small', 'MobileNet_V3_Large', 'ViT_B_16', 'ViT_B_32'\n",
    "]\n",
    "\n",
    "available_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12NNSwRK-dP7"
   },
   "source": [
    "## Data loading\n",
    "1. loading all paths to files with corresponding class    \n",
    "2. if npy already exists, then load it, otherwise load the dataset in memory  and save it as npy  \n",
    "3. load npy  \n",
    "4. split it as train, test, validation sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:45.006210817Z",
     "start_time": "2023-05-28T09:44:44.902718938Z"
    },
    "executionInfo": {
     "elapsed": 53375,
     "status": "ok",
     "timestamp": 1671181212044,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "erAlgBp9-K7j"
   },
   "outputs": [],
   "source": [
    "# Specify which subdataset to use: DATASET_GROUP_IDX = None or an index from 0 to 2\n",
    "DATASET_GROUP_IDX = None\n",
    "\n",
    "# Available subdatasets\n",
    "independent_dataset = [18, 9, 26, 29, 34]\n",
    "\n",
    "dataset_groups = [\n",
    "    [1, 7, 10, 14, 11, 32, 13, 4, 8, 17, 20, 25, 28, 31, 36],\n",
    "    [0, 15, 21, 22, 23, 5] + independent_dataset,\n",
    "    [37, 33, 16, 2, 3, 6, 12, 19, 24, 27, 30, 35],\n",
    "]\n",
    "\n",
    "# Stats about the data\n",
    "image_files = []\n",
    "targets = []\n",
    "\n",
    "# Dictionary to map class names to class indices\n",
    "class_dict = {class_name: i for i, class_name in enumerate(class_list) if not class_name.startswith('.')}\n",
    "\n",
    "# Load all image paths\n",
    "for class_name in class_dict.keys():\n",
    "    repetitions_list = os.listdir(os.path.join(extracted_data_path, class_name))\n",
    "    repetitions_list.sort()\n",
    "    for repetition in repetitions_list:\n",
    "        if repetition.startswith('.'):\n",
    "            continue\n",
    "        image_list = os.listdir(os.path.join(extracted_data_path, class_name, repetition))\n",
    "        image_list.sort()\n",
    "        image_files.extend([os.path.join(extracted_data_path, class_name, repetition, img) for img in image_list])\n",
    "        targets.extend([class_dict[class_name]] * len(image_list))\n",
    "\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Filter data based on selected dataset group\n",
    "if DATASET_GROUP_IDX is not None:\n",
    "    dataset_groups = [sorted(group) for group in dataset_groups]\n",
    "    assert 0 <= DATASET_GROUP_IDX < len(dataset_groups), 'Invalid DATASET_GROUP_IDX value'\n",
    "\n",
    "    mapper_allcls_to_subcls = {j: i for i, j in enumerate(dataset_groups[DATASET_GROUP_IDX])}\n",
    "    class_dict = {class_list[j]: i for i, j in enumerate(dataset_groups[DATASET_GROUP_IDX])}\n",
    "    class_list = [class_list[j] for j in dataset_groups[DATASET_GROUP_IDX]]\n",
    "\n",
    "    new_targets = []\n",
    "    new_image_files = []\n",
    "    for target, image_file in zip(targets, image_files):\n",
    "        if target in mapper_allcls_to_subcls:\n",
    "            new_targets.append(mapper_allcls_to_subcls[target])\n",
    "            new_image_files.append(image_file)\n",
    "    targets = np.array(new_targets)\n",
    "    image_files = new_image_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:45.323757712Z",
     "start_time": "2023-05-28T09:44:45.306679051Z"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1671181212046,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "V38N8QwdKmBQ"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def _fix_images_paths(images_paths, targets):\n",
    "    # Create deep copies of input lists\n",
    "    images_paths = deepcopy(images_paths)\n",
    "    targets = deepcopy(targets)\n",
    "    \n",
    "    # Trim the beginning of the lists until they align with the second element\n",
    "    while os.path.basename(images_paths[0]).split('_')[1:4] != os.path.basename(image_files[1]).split('_')[1:4]:\n",
    "        images_paths = images_paths[1:]\n",
    "        targets = targets[1:]\n",
    "    \n",
    "    # Trim the end of the lists until they align with the second-to-last element\n",
    "    while os.path.basename(images_paths[-1]).split('_')[1:4] != os.path.basename(image_files[-2]).split('_')[1:4]:\n",
    "        images_paths = images_paths[:-1]\n",
    "        targets = targets[:-1]\n",
    "    \n",
    "    return images_paths, targets\n",
    "\n",
    "def prepare_dataset(image_files, targets, train_valid_test_split, fix_len=10):\n",
    "    # Fix image paths and targets alignment\n",
    "    images_paths, targets_cls = _fix_images_paths(image_files, targets)\n",
    "\n",
    "    # Organize data into a dictionary by class and repetition\n",
    "    data_as_dict = {}\n",
    "    for el, _trg in zip(images_paths, targets_cls):\n",
    "        _cls, _rep_name = el.split(os.sep)[-3:-1]\n",
    "        k = (_cls, _rep_name)\n",
    "        if k not in data_as_dict:\n",
    "            data_as_dict[k] = []\n",
    "        data_as_dict[k].append((el, _trg))\n",
    "\n",
    "    # Ensure each class-repetition combination has a length divisible by fix_len\n",
    "    for k in data_as_dict:\n",
    "        while len(data_as_dict[k]) % fix_len != 0:\n",
    "            data_as_dict[k].append(data_as_dict[k][-1])\n",
    "    \n",
    "    # Prepare data for training, validation, and testing\n",
    "    datas = {}\n",
    "    for k in train_valid_test_split:\n",
    "        for rep_name in train_valid_test_split[k]:\n",
    "            if (k, rep_name) not in data_as_dict:\n",
    "                continue\n",
    "            data_type = train_valid_test_split[k][rep_name]\n",
    "            if data_type not in datas:\n",
    "                datas[data_type] = []\n",
    "            files_in_rep = len(data_as_dict[(k, rep_name)])\n",
    "            for start_idx in range(0, files_in_rep - fix_len + 1, 2):\n",
    "                seq = data_as_dict[(k, rep_name)][start_idx:start_idx + fix_len]\n",
    "                assert len(seq) == fix_len, '...'\n",
    "                datas[data_type].append(seq)\n",
    "                \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:47.444382890Z",
     "start_time": "2023-05-28T09:44:47.379354855Z"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1671181212048,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "oHXGVaw1MdXp"
   },
   "outputs": [],
   "source": [
    "train_valid_test_split_json_name = 'NEW25split.json' \n",
    "with open(os.path.join(root_path, train_valid_test_split_json_name), 'r') as f:\n",
    "    train_valid_test_split = json.load(f)\n",
    "    \n",
    "datas = prepare_dataset(image_files, targets, train_valid_test_split, fix_len=max(SEQ_LEN, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:47.557290515Z",
     "start_time": "2023-05-28T09:44:47.420841948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1671181212049,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "WaQn5fkhMfng",
    "outputId": "9cb311a0-1636-4836-b047-1633d4dc0fc1"
   },
   "outputs": [],
   "source": [
    "from pytorch_utils.data_utils import *\n",
    "\n",
    "USE_AUGMENTATION = False\n",
    "\n",
    "# loading data\n",
    "train_dataset = ClassificationPlantSequenceDataset(datas['train'], use_augmentation=USE_AUGMENTATION)\n",
    "val_dataset = ClassificationPlantSequenceDataset(datas['valid'])\n",
    "test_dataset = ClassificationPlantSequenceDataset(datas['test'])\n",
    "\n",
    "# combine val and test\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset, test_dataset])\n",
    "test_dataset = val_dataset\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "print('Data loaders created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:54.006475431Z",
     "start_time": "2023-05-28T09:44:49.534705032Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 159854,
     "status": "ok",
     "timestamp": 1671181371883,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "QtLcvfmtMw1u",
    "outputId": "05d13107-9fb6-47fc-c4a6-b90be31a511c"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "dataset = ClassificationPlantSequenceDataset(datas['train'], use_augmentation=USE_AUGMENTATION)\n",
    "\n",
    "idxs_in_batch = [0,1,2]\n",
    "sample_size = 10\n",
    "\n",
    "for idx_in_batch in idxs_in_batch:\n",
    "    img_idx = np.random.randint(0, len(dataset), size=sample_size)\n",
    "    sample = [dataset[i] for i in img_idx]\n",
    "    imgs = sample[idx_in_batch][0].cpu()\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(sample_size):\n",
    "        img = imgs[:,i,:,:].permute(1, 2, 0)\n",
    "        plt.subplot(1, sample_size, i + 1)\n",
    "        plt.imshow((img * 255).type(torch.uint8))\n",
    "        plt.title('Class: {}'.format(class_list[sample[idx_in_batch][1]]))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7mGbOA21QiJ"
   },
   "source": [
    "#### Dataset class and data loaders\n",
    "By loading the whole dataset inside the memory, it because very easy to train the model. All we have to do is normalise the images (divide by 255 and change into C, W, H format) then return it with it's corresponding target class\n",
    "\n",
    "- Data augmentation can be turned on/off by modifying this code (more details in comments at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxKuuukRC9f3"
   },
   "source": [
    "### Data sample (Run this only if you want to see an example of the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gGak8ErYs48"
   },
   "source": [
    "### LR Scheduler and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:54.019251671Z",
     "start_time": "2023-05-28T09:44:54.005716743Z"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1671181371886,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "2Dmhcs-AYtFY"
   },
   "outputs": [],
   "source": [
    "from pytorch_utils.training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-jWW_2O_Yoy"
   },
   "source": [
    "## Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:56.493167768Z",
     "start_time": "2023-05-28T09:44:56.487220495Z"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1671181371888,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "SP6pjJzJzmH5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _handlezero_division_np(a,b):\n",
    "    # initialize output tensor with desired value\n",
    "    # c = torch.zeros_like(a)\n",
    "    #c = torch.full_like(a, fill_value=float('nan'))\n",
    "    # zero mask\n",
    "    c = np.zeros_like(a)\n",
    "    mask = (b != 0)\n",
    "    # finally perform division\n",
    "    c[mask] = a[mask] / b[mask]\n",
    "    return c\n",
    "\n",
    "def mathews_correlation_coefficient_np(tp, fp, fn, tn):\n",
    "    tp = tp.sum().astype(np.float64)\n",
    "    tn = tn.sum().astype(np.float64)\n",
    "    fp = fp.sum().astype(np.float64)\n",
    "    fn = fn.sum().astype(np.float64)\n",
    "    _numerator = (tp*tn - fp*fn)\n",
    "    _denomerator = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    x = _numerator / (_denomerator + 1e-11)\n",
    "    # x = _handlezero_division_np(_numerator, _denomerator)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:58.783493144Z",
     "start_time": "2023-05-28T09:44:58.778048955Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_utils.callbacks as pt_callbacks\n",
    "\n",
    "def get_callbacks(\n",
    "        optimiser,\n",
    "        result,\n",
    "        model,\n",
    "        defined_callbacks=None,\n",
    "        continue_training=False,\n",
    "        other_stats=None\n",
    "):\n",
    "\n",
    "    if defined_callbacks is None:\n",
    "        defined_callbacks = {\n",
    "            'val': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                          model_save_path=model_save_path + 'model.pth',\n",
    "                                          training_stats_path=model_save_path + 'training_stats_val',\n",
    "                                          continue_training=continue_training),\n",
    "\n",
    "            'train': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                            training_stats_path=model_save_path + 'training_stats_train',\n",
    "                                            continue_training=continue_training)\n",
    "        }\n",
    "\n",
    "    defined_callbacks['val'].reduce_lr_on_plateau(\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        indicator_text=\"Val LR scheduler: \"\n",
    "    )\n",
    "    defined_callbacks['val'].model_checkpoint(\n",
    "        model=model,\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        indicator_text=\"Val checkpoint: \"\n",
    "    )\n",
    "    stop_flag = defined_callbacks['val'].early_stopping(\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        patience=25,\n",
    "        indicator_text=\"Early stopping: \"\n",
    "    )\n",
    "    defined_callbacks['val'].clear_memory()\n",
    "    print(\"_________\")\n",
    "\n",
    "    return defined_callbacks, stop_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training loop and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:45:00.905183280Z",
     "start_time": "2023-05-28T09:45:00.838690120Z"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1671181371890,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "B7Y8z2bk-RiV"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_utils.training_utils as pt_train\n",
    "\n",
    "def train_loop(\n",
    "        model,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        model_save_folder=model_save_path,\n",
    "        initial_lr=0.001,\n",
    "        weight_decay=None,\n",
    "        running_hyperopt=False,\n",
    "        verbose=False,\n",
    "        continue_training=False,\n",
    "):\n",
    "    def get_result_list(history, metric):\n",
    "        return [history[i][metric] for i in range(len(history))]\n",
    "\n",
    "    # prep the model save path\n",
    "    shutil.rmtree(model_save_folder, ignore_errors=True)\n",
    "    os.makedirs(model_save_folder, exist_ok=True)\n",
    "\n",
    "    # Train the model using torch\n",
    "    history = pt_train.fit(\n",
    "        epochs=epochs,\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay,\n",
    "        model=model,\n",
    "        continue_training=continue_training,\n",
    "        callbacks_function=get_callbacks,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        opt_func=optimizer,\n",
    "    )\n",
    "\n",
    "    del model\n",
    "\n",
    "    # load the best model from checkpoint\n",
    "    model = torch.load(model_save_path + \"model.pth\")\n",
    "\n",
    "    train_loss_history = get_result_list(history, \"train_loss\")\n",
    "    train_acc_history = get_result_list(history, \"train_acc\")\n",
    "    val_loss_history = get_result_list(history, \"val_loss\")\n",
    "    val_acc_history = get_result_list(history, \"val_acc\")\n",
    "\n",
    "    return model, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, verbose=True, eps=1e-10):\n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    confusion_matrix = ConfusionMatrix(num_classes=len(class_list))\n",
    "    eval_preds = list()\n",
    "    eval_targs = list()\n",
    "\n",
    "    # computing predictions and confusion matrix\n",
    "    for i, (images, targets) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "        images, targets = images.to(device, dtype=torch.float), torch.Tensor(targets).to(device)\n",
    "        outputs = torch.nn.functional.log_softmax(model(images), dim=1)\n",
    "        preds = torch.argsort(outputs, dim=1, descending=True)[:, :3]\n",
    "        eval_preds.extend(preds[:, 0].cpu().numpy())\n",
    "        eval_targs.extend(targets.cpu().numpy())\n",
    "\n",
    "    # computing main metrics (acc, precisio, recall and f1 score)\n",
    "    matrix = confusion_matrix(torch.tensor(eval_preds), torch.tensor(eval_targs))\n",
    "    accuracy = matrix.trace() / (matrix.sum()+eps)\n",
    "    # loss = F.cross_entropy(torch.tensor(eval_preds), torch.tensor(eval_targs))\n",
    "    precision = np.array([matrix[i, i] / (matrix.sum(axis=0)[i]+eps) for i in range(len(class_list))])\n",
    "    recall = np.array([matrix[i, i] / (matrix.sum(axis=1)[i]+eps) for i in range(len(class_list))])\n",
    "    f1_score = 2 * precision * recall / (precision + recall+eps)\n",
    "\n",
    "    # computing false positive rate, false negative rate, false discovery rate, false omission rate\n",
    "    fp_rate = np.zeros(len(class_list))\n",
    "    for idx in range(len(class_list)):\n",
    "        tn = matrix.trace() - matrix[idx, idx]\n",
    "        fp = np.sum([matrix[j, idx] for j in range(len(class_list)) if j != idx])\n",
    "        fp_rate[idx] = fp / (fp + tn+eps)\n",
    "\n",
    "    fn_rate = 1 - recall\n",
    "    fd_rate = 1 - precision\n",
    "    specificity = 1 - fp_rate\n",
    "\n",
    "    fo_rate = np.zeros(len(class_list))\n",
    "    for idx in range(len(class_list)):\n",
    "        n = np.sum(np.array(eval_targs) != idx)\n",
    "        fn = np.sum([matrix.sum(axis=0)[j] - matrix[j, j] for j in range(len(class_list)) if j != idx])\n",
    "        fo_rate[idx] = fn / (n+eps)\n",
    "\n",
    "    missclassification_rate = 1 - accuracy\n",
    "    npv = 1 - fo_rate\n",
    "\n",
    "    mcc_per_class = []\n",
    "    for idx in range(len(class_list)):\n",
    "        tp = matrix[idx, idx].cpu().numpy()\n",
    "        tn = (matrix.trace() - matrix[idx, idx]).cpu().numpy()\n",
    "        fp = np.sum([matrix[j, idx] for j in range(len(class_list)) if j != idx])\n",
    "        fn = np.sum([matrix.sum(axis=0)[j] - matrix[j, j] for j in range(len(class_list)) if j != idx])\n",
    "        _mcc = mathews_correlation_coefficient_np(tp, fp, fn, tn)\n",
    "        mcc_per_class.append(_mcc)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Accuracy: {:.3f}%'.format(accuracy * 100))\n",
    "        # print('Loss: {:.3f}'.format(loss))\n",
    "        print('Average precision: {:.3f}'.format(precision.mean()))\n",
    "        print('Average recall: {:.3f}'.format(recall.mean()))\n",
    "        print('Average F1 score: {:.3f}'.format(f1_score.mean()))\n",
    "        print('Average specificity: {:.3f}'.format(specificity.mean()))\n",
    "        print('Average false positive rate: {:3f}'.format(fp_rate.mean()))\n",
    "        print('Average false negative rate: {:3f}'.format(fn_rate.mean()))\n",
    "        print('Average false discovery rate: {:.3f}'.format(fd_rate.mean()))\n",
    "        print('Average false omission rate: {:.3f}'.format(fo_rate.mean()))\n",
    "        print('Missclassification rate: {:.2f}%'.format(missclassification_rate * 100))\n",
    "        print('Mathews Correlation Coefficient: {:.2f}'.format(np.mean(mcc_per_class)))\n",
    "        print('--------------------------------------------')\n",
    "        print('Results by class :')\n",
    "        print('--------------------------------------------')\n",
    "        print('{:<15}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}'.format('', 'Precision', 'Recall', 'F1 score', 'Specificity', 'FPR', 'FNR', 'FDR', 'FOR', 'NPV', 'MCC'))\n",
    "        for idx, class_name in enumerate(class_list):\n",
    "            print('{:<15}{:<12.2f}{:<12.2f}{:<12.2f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}'.format(\n",
    "                class_name, precision[idx], recall[idx], f1_score[idx], specificity[idx], fp_rate[idx], fn_rate[idx], fd_rate[idx], fo_rate[idx], npv[idx], mcc_per_class[idx]\n",
    "            ))\n",
    "        print('--------------------------------------------')\n",
    "        print()\n",
    "\n",
    "        # ploting confusion matrix\n",
    "        matrix_df = pd.DataFrame(matrix.numpy(), index=class_list, columns=class_list)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sn.heatmap(matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "    return accuracy, precision, recall, f1_score, matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:45:02.790338493Z",
     "start_time": "2023-05-28T09:45:02.776264278Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(\n",
    "        outputs: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom accuracy function to override the default one in pt_train\n",
    "    \"\"\"\n",
    "\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class CustomModelBase(pt_train.CustomModelBase):\n",
    "    \"\"\"\n",
    "    ModelBase override for training and validation steps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None):\n",
    "        super(CustomModelBase, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.accuracy_function = accuracy\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels, weight=self.class_weights)  # Calculate loss with class weights\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels, weight=self.class_weights)  # Calculate loss with class weights\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt for finding best combination of hyperparameters\n",
    "\n",
    "Notes:\n",
    "1. All models are using Adam as the standard optimizer. You can explore on other optimizer by enabling the options in the search space.\n",
    "\n",
    "Reference:\n",
    "https://github.com/hyperopt/hyperopt/wiki/FMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:53:44.896521190Z",
     "start_time": "2023-05-28T09:53:44.749869024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# R3D_18\n",
    "# 4M parameters\n",
    "class R3D_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(R3D_18, self).__init__()\n",
    "        self.model = models.video.r3d_18(weights=models.video.R3D_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# mc3_18\n",
    "# 4M parameters\n",
    "class MC3_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MC3_18, self).__init__()\n",
    "        self.model = models.video.mc3_18(weights=models.video.MC3_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# r2plus1d_18\n",
    "# 4M parameters\n",
    "class r2plus1d_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(r2plus1d_18, self).__init__()\n",
    "        self.model = models.video.r2plus1d_18(weights=models.video.R2Plus1D_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4M parameters\n",
    "# no init weights\n",
    "class Eff_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=False)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "        h0 = torch.zeros(self.num_layers, embeddings.shape[1], self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, embeddings.shape[1], self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff2_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff2_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=0.5)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(MNV3S_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(MNV3S_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(MNV3S_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(MNV3S_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R18_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R18_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R18_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R18_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R34_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R34_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R34_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R34_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "# ---\n",
    "\n",
    "# 4M parameters\n",
    "class R50_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R50_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R50_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R50_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R50_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "#-----------------------------#\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class R101_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R101_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R101_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R101_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R101_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "from vivit_modules import ViViT\n",
    "# ViViT Small\n",
    "ViViT_Small = ViViT(heads=3, depth=4, dim=192)\n",
    "\n",
    "# ViViT Large\n",
    "ViViT_Large = ViViT(heads=12, depth=24, dim=768)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:15.789857470Z",
     "start_time": "2023-05-28T10:30:15.709135236Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DROPOUT = 0.0\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "PRETRAINED_NUM_CLASSES = len(class_list)  # default was len(class_list)\n",
    "#--------------------------------\n",
    "# EfficientNetB1 pretrained\n",
    "#--------------------------------\n",
    "\n",
    "class EfficientNetB1(CustomModelBase):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetB1, self).__init__()\n",
    "        self.model = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff1_pretrained(EfficientNetB1):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}eff_b1.pth\"\n",
    "    ):\n",
    "        super(Eff1_pretrained, self).__init__()\n",
    "        # self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_b1().classifier[-1].in_features\n",
    "\n",
    "        # self.out_features = self.model.fc.out_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff1_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff1_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Eff1_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff1_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class Eff1_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff1_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff1_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff1_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "#--------------------------------\n",
    "# EfficientNetB2 pretrained\n",
    "#--------------------------------\n",
    "\n",
    "class EfficientNetB2(CustomModelBase):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetB2, self).__init__()\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff2_pretrained(EfficientNetB2):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}eff_b2.pth\"\n",
    "    ):\n",
    "        super(Eff2_pretrained, self).__init__()\n",
    "        # self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_b2().classifier[-1].in_features\n",
    "\n",
    "        # self.out_features = self.model.fc.out_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff2_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff2_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Eff2_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff2_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "#---------------------------------\n",
    "# EfficientNetV2\n",
    "#---------------------------------\n",
    "\n",
    "class EfficientNetV2_S(nn.Module):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetV2_S, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EffV2_S_pretrained(EfficientNetV2_S):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}effv2_s.pth\"\n",
    "    ):\n",
    "        super(EffV2_S_pretrained, self).__init__()\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_v2_s().classifier[-1].in_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EffV2_S_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * 2 * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * 2 * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:18.055838367Z",
     "start_time": "2023-05-28T10:30:18.043302546Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model_by_name(name):\n",
    "    model_directory = {\n",
    "        \"R3D_18\": R3D_18,\n",
    "        \"MC3_18\": MC3_18,\n",
    "        \"r2plus1d_18\": r2plus1d_18,\n",
    "        \"Eff_LSTM\": Eff_LSTM,\n",
    "        \"Eff_GRU\": Eff_GRU,\n",
    "        \"Eff_BiGRU\": Eff_BiGRU,\n",
    "        \"Eff_BiLSTM\": Eff_BiLSTM,\n",
    "        \"Eff2_LSTM\": Eff2_LSTM,\n",
    "        \"Eff2_GRU\": Eff2_GRU,\n",
    "        \"Eff2_BiLSTM\": Eff2_BiLSTM,\n",
    "        \"Eff2_BiGRU\": Eff2_BiGRU,\n",
    "        \"MNV3S_LSTM\": MNV3S_LSTM,\n",
    "        \"MNV3S_GRU\": MNV3S_GRU,\n",
    "        \"MNV3S_BiLSTM\": MNV3S_BiLSTM,\n",
    "        \"MNV3S_BiGRU\": MNV3S_BiGRU,\n",
    "        \"R18_LSTM\": R18_LSTM,\n",
    "        \"R18_GRU\": R18_GRU,\n",
    "        \"R18_BiLSTM\": R18_BiLSTM,\n",
    "        \"R18_BiGRU\": R18_BiGRU,\n",
    "        \"R34_LSTM\": R34_LSTM,\n",
    "        \"R34_GRU\": R34_GRU,\n",
    "        \"R34_BiLSTM\": R34_BiLSTM,\n",
    "        \"R34_BiGRU\": R34_BiGRU,\n",
    "        \"R50_LSTM\": R50_LSTM,\n",
    "        \"R50_GRU\": R50_GRU,\n",
    "        \"R50_BiLSTM\": R50_BiLSTM,\n",
    "        \"R50_BiGRU\": R50_BiGRU,\n",
    "        \"R101_LSTM\": R101_LSTM,\n",
    "        \"R101_GRU\": R101_GRU,\n",
    "        \"R101_BiLSTM\": R101_BiLSTM,\n",
    "        \"R101_BiGRU\": R101_BiGRU,\n",
    "        \"Eff1_GRU_pretrained\": Eff1_GRU_pretrained,\n",
    "        \"Eff1_LSTM_pretrained\": Eff1_LSTM_pretrained,\n",
    "        \"Eff1_BiGRU_pretrained\": Eff1_BiGRU_pretrained,\n",
    "        \"Eff1_BiLSTM_pretrained\": Eff1_BiLSTM_pretrained,\n",
    "        \"Eff2_GRU_pretrained\": Eff2_GRU_pretrained,\n",
    "        \"Eff2_LSTM_pretrained\": Eff2_LSTM_pretrained,\n",
    "        \"Eff2_BiGRU_pretrained\": Eff2_BiGRU_pretrained,\n",
    "        \"Eff2_BiLSTM_pretrained\": Eff2_BiLSTM_pretrained,\n",
    "        \"EffV2_S_GRU_pretrained\": EffV2_S_GRU_pretrained,\n",
    "        \"EffV2_S_LSTM_pretrained\": EffV2_S_LSTM_pretrained,\n",
    "        \"EffV2_S_BiGRU_pretrained\": EffV2_S_BiGRU_pretrained,\n",
    "        \"EffV2_S_BiLSTM_pretrained\": EffV2_S_BiLSTM_pretrained,\n",
    "        \"ViViT_Small\": ViViT_Small,\n",
    "        \"ViViT_Large\": ViViT_Large\n",
    "    }\n",
    "    return model_directory[name]\n",
    "\n",
    "def get_optimizer_by_name(optim):\n",
    "    if optim == 'SGD':\n",
    "        optimizer = torch.optim.SGD #(model.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-6)\n",
    "    elif optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam #(model.parameters(), lr=0.02, weight_decay=1e-6)\n",
    "    elif optim == 'RMSProp':\n",
    "        optimizer = torch.optim.RMSprop #(model.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-6)\n",
    "    elif optim == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW #(model.parameters(), lr=0.02, weight_decay=1e-6)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:20.645062438Z",
     "start_time": "2023-05-28T10:30:20.632931325Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# define a search space\n",
    "from datetime import datetime\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, space_eval, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "\n",
    "def train_model(kwargs):\n",
    "\n",
    "    print(kwargs)\n",
    "    epochs = kwargs.get(\"epochs\", EPOCHS)\n",
    "\n",
    "    # get model by name\n",
    "    model_name = kwargs.get(\"model\")\n",
    "    model_cls = get_model_by_name(model_name)\n",
    "    model = model_cls(num_classes=len(class_list)).to(device)\n",
    "\n",
    "    # get optimizer\n",
    "    optim_args = kwargs.get(\"optim\")\n",
    "    print(optim_args[\"params\"])\n",
    "    optimizer_cls = get_optimizer_by_name(optim_args.get(\"name\"))\n",
    "\n",
    "    model, train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_loop(\n",
    "        model,\n",
    "        optimizer_cls,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        initial_lr=optim_args[\"params\"][\"lr\"],\n",
    "        weight_decay=optim_args[\"params\"][\"weight_decay\"],\n",
    "        verbose=True,\n",
    "        running_hyperopt=True,\n",
    "        continue_training=False\n",
    "    )\n",
    "\n",
    "    return model, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "\n",
    "def train_model_hyperopt(kwargs):\n",
    "\n",
    "    model, train_loss_history, train_acc_history, val_loss_history, val_acc_history  = train_model(kwargs)\n",
    "\n",
    "    return {\"loss\": np.mean(val_loss_history), \"status\": STATUS_OK}\n",
    "\n",
    "def unpack_values(trial):\n",
    "    vals = trial[\"misc\"][\"vals\"]\n",
    "    # unpack the one-element lists to values\n",
    "    # and skip over the 0-element lists\n",
    "    rval = {}\n",
    "    for k, v in list(vals.items()):\n",
    "        if v:\n",
    "            rval[k] = v[0]\n",
    "    return rval\n",
    "\n",
    "def export_hyperopt_log(trials):\n",
    "    result_list = []\n",
    "    for trial in trials.trials:\n",
    "        trial_result = space_eval(search_space, unpack_values(trial))\n",
    "        trial_result[\"val_loss\"] = trial['result']['loss']\n",
    "        result_list.append(trial_result)\n",
    "\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    df_result = pd.concat([df_result.drop(\"optim\", axis=1), pd.json_normalize(df_result.optim)], axis=1)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"../data/log\"\n",
    "    output_path = os.path.join(output_dir, f\"hyperopt_result_{ts}.csv\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"Exported hyperopt log to {output_path}\")\n",
    "    return df_result\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"epochs\": scope.int(hp.choice(\"epochs\", [7, 10, 13, 15, 17, 20, 22, 25, 27, 30, 33, 35])),\n",
    "    \"model\": hp.choice(\"model_name\", [\n",
    "         \"R3D_18\",\n",
    "         \"MC3_18\",\n",
    "         \"r2plus1d_18\",\n",
    "        # \"Eff_LSTM\",\n",
    "        \"Eff_GRU\",\n",
    "        # \"Eff_BiGRU\",\n",
    "        # \"Eff_BiLSTM\",\n",
    "        #\"Eff2_LSTM\",\n",
    "        \"Eff2_GRU\",\n",
    "        #\"Eff2_BiLSTM\",\n",
    "        #\"Eff2_BiGRU\",\n",
    "        # \"MNV3S_LSTM\",\n",
    "        \"MNV3S_GRU\",\n",
    "        # \"MNV3S_BiLSTM\",\n",
    "        # \"MNV3S_BiGRU\",\n",
    "        # \"R18_LSTM\",\n",
    "         #\"R18_GRU\",\n",
    "        # \"R18_BiLSTM\",\n",
    "        # \"R18_BiGRU\",\n",
    "        # \"R34_LSTM\",\n",
    "         #\"R34_GRU\",\n",
    "        # \"R34_BiLSTM\",\n",
    "        # \"R34_BiGRU\",\n",
    "        #\"R50_LSTM\",\n",
    "        \"R50_GRU\",\n",
    "        # \"R50_BiLSTM\",\n",
    "        # \"R50_BiGRU\",\n",
    "        # \"R101_LSTM\",\n",
    "        \"R101_GRU\",\n",
    "        # \"R101_BiLSTM\",\n",
    "        #\"R101_BiGRU\",\n",
    "        \"ViViT_small\",\n",
    "        \"ViViT_large\"\n",
    "    ]),\n",
    "    \"optim\": hp.choice(\"optim\",[\n",
    "        {\n",
    "            \"name\":\"Adam\",\n",
    "            \"params\": {\n",
    "                \"lr\": hp.choice(\"lr-3\", [1e-3, 1e-4]),\n",
    "                \"weight_decay\": hp.choice(\"weight_decay-3\", [3.310305423548208e-05])\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the average train loss over the space\n",
    "trials = Trials()\n",
    "max_evals = 1 if RUN_MODE == \"DEV\" else 192\n",
    "best = fmin(train_model_hyperopt, search_space, algo=tpe.suggest, max_evals=max_evals, trials=trials, verbose=False)\n",
    "print(space_eval(search_space, best))\n",
    "\n",
    "# export log\n",
    "export_hyperopt_log(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:36.958247158Z",
     "start_time": "2023-05-28T10:30:30.711961323Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model with the best parameter\n",
    "best_params = {'epochs': 1000000, 'model': 'Eff1_GRU_pretrained', 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.001, 'weight_decay': 3.310305423548208e-05}}}\n",
    "#best_params = space_eval(search_space, best) #comment in case of changing parameters\n",
    "best_model, train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(best_params)\n",
    "torch.save(best_model, \"seq_images.pth\")\n",
    "\n",
    "plot_model_stats(type(best_model).__name__, train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n",
    "evaluate_model(best_model, test_loader, verbose=True, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_model = torch.load(model_save_path + \"model.pth\")\n",
    "evaluate_model(best_model, test_loader, verbose=True, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T17:52:29.575693Z",
     "start_time": "2023-05-06T17:52:29.546693Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def get_all_conv_layers(model, modules_list=None, conv_layers=[], depth=0, grad_cam=False, feature_map=False):\n",
    "    \"\"\"\n",
    "    Get all the convolutional layers of a given model\n",
    "    \"\"\"\n",
    "    if modules_list is None:\n",
    "        modules_list = list(model.modules())\n",
    "\n",
    "    # get all the conv layers so that the last layer is used for grad cam visualisation\n",
    "    if grad_cam and (not feature_map):\n",
    "        for layer in modules_list:\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                conv_layers.append(layer)\n",
    "            elif isinstance(layer, torch.nn.Sequential):\n",
    "                get_all_conv_layers(model, layer, conv_layers, depth=depth + 1)\n",
    "\n",
    "    # get all inner conv layers for feature map visualisation\n",
    "    elif feature_map and (not grad_cam):\n",
    "        for layer in modules_list:\n",
    "            if isinstance(layer, torch.nn.Conv2d) and depth > 0:\n",
    "                conv_layers.append(layer)\n",
    "            elif isinstance(layer, torch.nn.Sequential) and depth > 0:\n",
    "                get_all_conv_layers(model, layer, conv_layers, depth=depth + 1)\n",
    "\n",
    "    return conv_layers\n",
    "\n",
    "\n",
    "def visualise_feature_maps(feature_map, feature_map_name):\n",
    "    \"\"\"\n",
    "    Visualise the feature maps of a given layer\n",
    "    \"\"\"\n",
    "    feature_map = feature_map.cpu().numpy()\n",
    "\n",
    "    # Get the number of feature maps\n",
    "    num_feature_maps = feature_map.shape[1]\n",
    "\n",
    "    # Calculate the number of rows and columns for the plot\n",
    "    num_cols = 8\n",
    "    num_rows = num_feature_maps // num_cols + int(num_feature_maps % num_cols > 0)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "    plots = []\n",
    "    for i in range(num_feature_maps):\n",
    "        ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.imshow(feature_map[0, i], cmap=\"viridis\")\n",
    "        ax.axis(\"off\")\n",
    "        plots.append(feature_map[0, i])\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(num_feature_maps, num_rows * num_cols):\n",
    "        axes[i // num_cols, i % num_cols].axis(\"off\")\n",
    "\n",
    "    plt.savefig(feature_map_name)\n",
    "    plt.close('all')\n",
    "\n",
    "    # return the figure\n",
    "    return plots\n",
    "\n",
    "\n",
    "def normalize_feature_map(feature_map):\n",
    "    min_val, max_val = np.min(feature_map), np.max(feature_map)\n",
    "    return (feature_map - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def visualise_gradcam(images_numpy, cam, ith_image, seq_idx, i, grad_img_i_folder, max_gradcam_images=3, show_gradcam=True, class_name=\"\"):\n",
    "    \"\"\"\n",
    "    save GradCAMs for the given image\n",
    "    \"\"\"\n",
    "\n",
    "    if show_gradcam and (i < max_gradcam_images):\n",
    "        print(f\"Extracting grad cam for image {i + 1}_{seq_idx}/{max_gradcam_images}\")\n",
    "\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images_numpy)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        grayscale_cam = cam(input_tensor=ith_image, targets=None)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(grayscale_cam)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        visualization = show_cam_on_image(images_numpy, grayscale_cam, use_rgb=True, image_weight=0.8)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(visualization)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(grad_img_i_folder + f\"image_{seq_idx + 1}_{class_name}.png\")\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "def show_all_feature_maps(target_layers, model, ith_image, i, seq_index, layers_folder):\n",
    "    \"\"\"\n",
    "    Show all the feature maps of all the target layers, of a given model, in a given image\n",
    "    \"\"\"\n",
    "\n",
    "    # get feature maps for each detected target layers\n",
    "    list_of_plots = {}\n",
    "    valid_target_layers = []\n",
    "\n",
    "    # Extract feature maps for each target layer and save them, if they are valid\n",
    "    cnt = 0\n",
    "    for j, layer in enumerate(target_layers):\n",
    "        feature_maps = []\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            feature_maps.append(output.detach())\n",
    "\n",
    "        # register hook to the layer to get the feature maps\n",
    "        layer.register_forward_hook(hook_fn)\n",
    "        model(ith_image)\n",
    "\n",
    "        # if no feature maps were found, skip this layer\n",
    "        if len(feature_maps) == 0:\n",
    "            layer._forward_hooks.clear()\n",
    "            continue\n",
    "\n",
    "        if feature_maps[0].shape[-1] <=1:\n",
    "            layer._forward_hooks.clear()\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracting feature maps for seq {i + 1}, image {seq_index}, from layer {j + 1}/{len(target_layers)}\")\n",
    "\n",
    "        # save feature maps using this function\n",
    "        plots = visualise_feature_maps(feature_maps[0], f\"{layers_folder}layer_{cnt + 1}.png\")\n",
    "        cnt += 1\n",
    "\n",
    "        # update the list of valid target layers and the list of plots\n",
    "        list_of_plots[j] = plots\n",
    "        valid_target_layers.append((layer, j))  # save the layer and its index\n",
    "\n",
    "        layer._forward_hooks.clear()\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "    return list_of_plots, valid_target_layers\n",
    "\n",
    "\n",
    "def create_chart(num_single_chart_layers, num_single_chart_conv_imgs, valid_target_layers, list_of_plots, layers_folder_prev, i):\n",
    "    \"\"\"\n",
    "    Create a chart with num_single_chart_layers layers and num_single_chart_conv_imgs feature maps per layer\n",
    "    \"\"\"\n",
    "\n",
    "    # select the smallest of chosen number of rows in the chart and number of valid target layers\n",
    "    num_single_chart_layers = min(num_single_chart_layers, len(valid_target_layers))\n",
    "\n",
    "    # pick num_single_chart_layers random layers without changing the order\n",
    "    remove_layers_numbers = random.sample(valid_target_layers, max(len(valid_target_layers) - num_single_chart_layers, 0))\n",
    "    valid_target_layers = [layer for layer in valid_target_layers if layer not in remove_layers_numbers]\n",
    "\n",
    "    # put all layers in one image\n",
    "    print(f\"Merging all layers in one image...\")\n",
    "    plt.figure(figsize=(10 * num_single_chart_conv_imgs, 10 * num_single_chart_layers))\n",
    "    cnt = 1\n",
    "\n",
    "    # merge all feature map plots in one image to form a chart\n",
    "    for j, layer in enumerate(valid_target_layers):\n",
    "        layer, layer_index = layer\n",
    "        plots = list_of_plots[layer_index]\n",
    "\n",
    "        # pick num_single_chart_conv_imgs random features per layer without changing the order\n",
    "        if num_single_chart_conv_imgs is not None:\n",
    "            plots_numbers = [random.randint(0, len(plots) - 1) for _ in range(num_single_chart_conv_imgs)]\n",
    "            plots_numbers = sorted(plots_numbers)\n",
    "            plots = [plots[i] for i in plots_numbers]\n",
    "        else:\n",
    "            num_single_chart_conv_imgs = len(plots)\n",
    "\n",
    "        # all plots in the selected layer\n",
    "        for k, plot in enumerate(plots):\n",
    "            plot = normalize_feature_map(plot)\n",
    "\n",
    "            subplot = plt.subplot(len(valid_target_layers), num_single_chart_conv_imgs, cnt)  # (*nrows*, *ncols*, *index*)\n",
    "            cnt += 1\n",
    "\n",
    "            plt.imshow(plot, cmap=\"viridis\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Add \"Row_j\" ylabel to the first subplot of each row\n",
    "            if k == 0:\n",
    "                label_axis = subplot.twinx()\n",
    "                label_axis.set_ylabel(f\"Layer_{layer_index + 1}\", fontsize=40, rotation=0, labelpad=160)\n",
    "                label_axis.yaxis.set_label_position(\"left\")\n",
    "                label_axis.yaxis.tick_left()\n",
    "                label_axis.yaxis.set_ticks([])\n",
    "                label_axis.xaxis.set_ticks([])\n",
    "\n",
    "    # plt.savefig(f\"feature_maps{os.sep}image_{i + 1}{os.sep}Chart.png\")\n",
    "    plt.savefig(layers_folder_prev + f\"Chart.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def get_gradcam_feature_maps(model, test_loader, show_gradcam=False, max_gradcam_images=5, show_feature_map=False, max_feature_map_images=3, max_feature_map_classes=2, num_feature_map_seqs_per_class=2, num_single_chart_layers=None, num_single_chart_conv_imgs=None, class_list=class_list):\n",
    "    \"\"\"\n",
    "    Compute GradCAM and feature maps for a given model and a given test_loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Get all conv layers from the given model and get the last layer to visualize in GradCAM\n",
    "    layers = get_all_conv_layers(model, feature_map=show_feature_map, grad_cam=show_gradcam)\n",
    "    target_layers = layers.copy()\n",
    "    layer = layers[-1]\n",
    "\n",
    "    if num_single_chart_layers is None:\n",
    "        num_single_chart_layers = len(target_layers)\n",
    "\n",
    "    cam = GradCAM(model=model, target_layers=[layer], use_cuda=True)\n",
    "\n",
    "    # create folders if they don't exist\n",
    "    if show_gradcam:\n",
    "        shutil.rmtree(\"gradcams\", ignore_errors=True)\n",
    "        os.makedirs(\"gradcams\")\n",
    "\n",
    "    if show_feature_map:\n",
    "        shutil.rmtree(\"feature_maps\", ignore_errors=True)\n",
    "        os.makedirs(\"feature_maps\")\n",
    "\n",
    "    # computing predictions and confusion matrix\n",
    "    class_seq_pairs = {}\n",
    "    for i, (images, targets) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "        # convert torch target to numpy and get the class name\n",
    "        targets = targets.numpy()\n",
    "        class_name = class_list[targets[0]]\n",
    "        print(\"Class name:\", class_name)\n",
    "        # continue\n",
    "\n",
    "        if show_feature_map:\n",
    "            max_reached_cnt = 0\n",
    "            for class_name_key in class_seq_pairs:\n",
    "                if len(class_seq_pairs[class_name_key]) >= num_feature_map_seqs_per_class:\n",
    "                    max_reached_cnt += 1\n",
    "\n",
    "            # stop execution when the maximum number of classes needed is reached and all classes are filled\n",
    "            if max_reached_cnt >= max_feature_map_classes:\n",
    "                print(f\"Max number of classes reached and filled: {max_feature_map_classes}\")\n",
    "                break\n",
    "\n",
    "            # skip iteration when the maximum number of classes needed is reached, but current classes are not yet filled\n",
    "            if (len(class_seq_pairs) >= max_feature_map_classes) and (class_name not in class_seq_pairs):\n",
    "                # print(f\"Max number of classes reached, seqs are still needed for current classes\")\n",
    "                continue\n",
    "\n",
    "            # assign the class name and sequence number used within it\n",
    "            if class_name not in class_seq_pairs:\n",
    "                class_seq_pairs[class_name] = [i]\n",
    "            else:\n",
    "                # reached max number of sequences needed for this class\n",
    "                if len(class_seq_pairs[class_name]) >= num_feature_map_seqs_per_class:\n",
    "                    print(f\"Max number of sequences reached for class {class_name}: {num_feature_map_seqs_per_class}\")\n",
    "                    continue\n",
    "                class_seq_pairs[class_name].append(i)\n",
    "\n",
    "        # for grad cams and feature maps create a separate folder for each image\n",
    "        if show_gradcam and i < max_gradcam_images:\n",
    "            grad_img_i_folder = f\"gradcams{os.sep}seq_{i + 1}{os.sep}\"\n",
    "            shutil.rmtree(grad_img_i_folder, ignore_errors=True)\n",
    "            os.makedirs(grad_img_i_folder)\n",
    "\n",
    "        print(f\"images shape\", images.shape)\n",
    "\n",
    "        images, targets = images.to(device, dtype=torch.float), torch.Tensor(targets).to(device)\n",
    "        org_images = images[0]  # we assume that batch size is 1, since it is designed to run on test loader\n",
    "\n",
    "        random_seq_idx = random.randint(0, images.shape[2] - 1 - max_feature_map_images)\n",
    "        num_feature_map_image_cnt = 0\n",
    "        for seq_idx in range(images.shape[2]):\n",
    "            if show_feature_map:\n",
    "                # we need 4 random consecutive images from the same sequence\n",
    "                if seq_idx < random_seq_idx:\n",
    "                    continue\n",
    "\n",
    "            # clear memory\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Get the ith image from the sequence\n",
    "            ith_image = org_images.permute(1, 0, 2, 3)  # ([3, 32, 128, 128]) to ([32, 3, 128, 128])\n",
    "            ith_image = ith_image[seq_idx:seq_idx + 1].clone()  # Create a new tensor to avoid modifying the original one\n",
    "\n",
    "            # No need to convert back to tensor since it's already a tensor\n",
    "            ith_image = ith_image.to(device, dtype=torch.float)\n",
    "            outputs = torch.nn.functional.log_softmax(model(ith_image), dim=1)\n",
    "            preds = torch.argsort(outputs, dim=1, descending=True)[:, :3]\n",
    "\n",
    "            # get numpy array from images\n",
    "            images_numpy = ith_image.cpu().numpy()\n",
    "            images_numpy = np.transpose(images_numpy, (0, 2, 3, 1))\n",
    "            images_numpy = np.squeeze(images_numpy)\n",
    "\n",
    "            # show GradCAMs for the max given images\n",
    "            if show_gradcam and (i < max_gradcam_images):\n",
    "                visualise_gradcam(\n",
    "                    images_numpy=images_numpy,\n",
    "                    cam=cam,\n",
    "                    ith_image=ith_image,\n",
    "                    seq_idx=seq_idx,\n",
    "                    i=i,\n",
    "                    max_gradcam_images=max_gradcam_images,\n",
    "                    show_gradcam=show_gradcam,\n",
    "                    grad_img_i_folder=grad_img_i_folder,\n",
    "                    class_name=class_name,\n",
    "                )\n",
    "\n",
    "            # show feature maps for the max given images\n",
    "            if show_feature_map:\n",
    "                # create folder for each image/sequence and delete the previous one\n",
    "                layers_folder = f\"feature_maps{os.sep}class_{class_name}{os.sep}seq_{len(class_seq_pairs[class_name])}{os.sep}image_{seq_idx + 1}{os.sep}layers{os.sep}\"\n",
    "                layers_folder_prev = layers_folder.replace(\"layers\" + os.sep, \"\")\n",
    "                shutil.rmtree(layers_folder_prev, ignore_errors=True)\n",
    "                os.makedirs(layers_folder, exist_ok=True)\n",
    "\n",
    "                # get feature maps for all layers\n",
    "                list_of_plots, valid_target_layers = show_all_feature_maps(\n",
    "                    target_layers=target_layers,\n",
    "                    model=model,\n",
    "                    ith_image=ith_image,\n",
    "                    i=i,\n",
    "                    seq_index=seq_idx + 1,\n",
    "                    layers_folder=layers_folder\n",
    "                )\n",
    "\n",
    "                # create chart for all layers of the current image\n",
    "                create_chart(\n",
    "                    num_single_chart_layers=num_single_chart_layers,\n",
    "                    num_single_chart_conv_imgs=num_single_chart_conv_imgs,\n",
    "                    valid_target_layers=valid_target_layers,\n",
    "                    list_of_plots=list_of_plots,\n",
    "                    layers_folder_prev=layers_folder_prev,\n",
    "                    i=i\n",
    "                )\n",
    "\n",
    "                num_feature_map_image_cnt += 1\n",
    "\n",
    "                print(f\"\\nFeature maps and chart for sequence {i + 1}, image {seq_idx + 1} saved successfully!\\n\\n\")\n",
    "                if num_feature_map_image_cnt >= max_feature_map_images and show_feature_map:\n",
    "                    print(f\"Max number of feature map images reached: {max_feature_map_images}\")\n",
    "                    break\n",
    "\n",
    "        # stop after max images\n",
    "        if ((i >= max_gradcam_images) and show_gradcam):\n",
    "            break\n",
    "\n",
    "    print(\"\\nMaximum selected sequences completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "* Make sure model files are in the folder `viz_models/`\n",
    "* When you run the cell, you'll be asked to choose the model you want to visualize. Enter the number of the model you want to visualize and press enter.\n",
    "* The selected model will run the images in the test set through the model and visualize the GradCAM and feature maps for the selected images.\n",
    "* You can select the number of images you want to run the GradCAM and feature maps for, by using the variables below. The images will be selected randomly from the test set.\n",
    "* Outputs will be in the folders `gradcams/` and `feature_maps/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "MODELS_FOLDER_NAME = \"vis_models\" + os.sep  # folder containing the model (.pth) files\n",
    "MAX_GRADCAM_SEQS = 60  # max number of sequences to show the GradCAM for\n",
    "\n",
    "NUM_FEATURE_MAP_CLASSES = 25  # number of classes to show the feature maps for\n",
    "MAX_FEATURE_MAP_SEQ_PER_CLASS = 1  # max random sequences per class\n",
    "\n",
    "NUM_SINGLE_CHART_CONV_IMGS = 5  # (N_COLUMNS) number of features in each layer, displayed as images in a single column of the \"feature maps chart\"\n",
    "NUM_SINGLE_CHART_LAYERS = 20  # (N_ROWS) number of random CNN layers, displayed as rows of the \"feature maps chart\"\n",
    "\n",
    "ENABLE_GRAD_CAM = True  # enable GradCAM visualization\n",
    "ENABLE_FEATURE_MAP = True  # enable feature map visualization\n",
    "#####################\n",
    "\n",
    "# load the list of models\n",
    "if not os.path.exists(MODELS_FOLDER_NAME):\n",
    "    os.makedirs(MODELS_FOLDER_NAME)\n",
    "\n",
    "    raise Exception(f\"Folder {MODELS_FOLDER_NAME} does not exist! The folder is created now, please put the model files in it and run the cell again\")\n",
    "\n",
    "# check if there are any model files in the folder\n",
    "if len(os.listdir(MODELS_FOLDER_NAME)) == 0:\n",
    "    raise Exception(f\"No model files found in {MODELS_FOLDER_NAME} folder! Please put the model files in it and run the cell again\")\n",
    "models_list = os.listdir(MODELS_FOLDER_NAME)\n",
    "\n",
    "# clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "string = \"Models available:\\n\"\n",
    "for i, model_name in enumerate(models_list):\n",
    "    string += f\"{i + 1}: {model_name}\\n\"\n",
    "\n",
    "# choose model\n",
    "ch = int(input(string + \"\\nChoose model: \"))\n",
    "model_name = MODELS_FOLDER_NAME + models_list[ch - 1]\n",
    "\n",
    "# load model\n",
    "model = torch.load(model_name)\n",
    "model.eval()\n",
    "\n",
    "# extract the CNN model only\n",
    "model = model.model\n",
    "\n",
    "MAX_FEATURE_MAP_IMGS = IMAGES_IN_A_DAY  # max number of images to show the feature maps for, per sequence. Layer wise visualization will be stored for 0 to n images in separate folders named as `feature_maps/image_i/seq_j/layer_k.png`\n",
    "if ENABLE_GRAD_CAM:\n",
    "    # extract and save grad cam\n",
    "    get_gradcam_feature_maps(\n",
    "        model,\n",
    "        test_loader,\n",
    "        show_gradcam=True,\n",
    "        show_feature_map=False,\n",
    "        max_gradcam_images=MAX_GRADCAM_SEQS,\n",
    "        max_feature_map_images=MAX_FEATURE_MAP_IMGS,\n",
    "        num_single_chart_conv_imgs=NUM_SINGLE_CHART_CONV_IMGS,\n",
    "        num_single_chart_layers=NUM_SINGLE_CHART_LAYERS,\n",
    "    )\n",
    "\n",
    "if ENABLE_FEATURE_MAP:\n",
    "    # extract and save feature maps and chart\n",
    "    get_gradcam_feature_maps(\n",
    "        model,\n",
    "        test_loader,\n",
    "        show_gradcam=False,\n",
    "        show_feature_map=True,\n",
    "        max_gradcam_images=MAX_GRADCAM_SEQS,\n",
    "        max_feature_map_images=MAX_FEATURE_MAP_IMGS,\n",
    "        num_feature_map_seqs_per_class=MAX_FEATURE_MAP_SEQ_PER_CLASS,\n",
    "        max_feature_map_classes=NUM_FEATURE_MAP_CLASSES,\n",
    "        num_single_chart_conv_imgs=NUM_SINGLE_CHART_CONV_IMGS,\n",
    "        num_single_chart_layers=NUM_SINGLE_CHART_LAYERS,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3mY7RGvk_Wwh"
   ],
   "machine_shape": "hm",
   "name": "",
   "provenance": [
    {
     "file_id": "1rqXdW8x2vLeIlkrv_ImgvhlvTFVVvaAg",
     "timestamp": 1665541249454
    }
   ],
   "version": ""
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cedb816d94db2648df1ed8d299ca42a0d191dc990e77464e1581386c54378e51"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00f6db93742340ae99cc7a770d2b5d45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fa67735a8ff40bb895bd93a28dc5d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b01080703a04600963164e9d21d395e",
      "placeholder": "​",
      "style": "IPY_MODEL_fd853d84518f403dabe1b9529b39cd80",
      "value": "100%"
     }
    },
    "218c85645a334a5f9501fe5fd1928613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48d014803cb34d1d97a556aa6a1ac39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e970d730079433ea0dbdee23564cb35",
      "placeholder": "​",
      "style": "IPY_MODEL_00f6db93742340ae99cc7a770d2b5d45",
      "value": "100%"
     }
    },
    "5e970d730079433ea0dbdee23564cb35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b01080703a04600963164e9d21d395e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82dbe6b8d15344b5a212d8a6d6da8d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9466404a296843098d47360f37c0ba92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98c00a5615c84119af4fe6b633417351": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0fa67735a8ff40bb895bd93a28dc5d80",
       "IPY_MODEL_c25e7a25081149c5a87f5d466a5fef1f",
       "IPY_MODEL_d88e2b9004504cf29da570d87b01af9b"
      ],
      "layout": "IPY_MODEL_9466404a296843098d47360f37c0ba92"
     }
    },
    "9a612f6b808b447e9b22c9ddf9bb8c58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b42a367042b4f41a0131827cc0f1295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a556607a3d8a41c2a54bcdd0545abd66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a78ee61e3f964c13b8b834f9f4dc6869": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7902ea5614649b28bc96388b1e56f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a612f6b808b447e9b22c9ddf9bb8c58",
      "placeholder": "​",
      "style": "IPY_MODEL_9b42a367042b4f41a0131827cc0f1295",
      "value": " 127M/127M [00:00&lt;00:00, 229MB/s]"
     }
    },
    "bda2d71f5d7e46babfeec1bcdf6abcbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9374215bd5d45189e506eee10340f82",
      "max": 133546016,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_218c85645a334a5f9501fe5fd1928613",
      "value": 133546016
     }
    },
    "c25e7a25081149c5a87f5d466a5fef1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82dbe6b8d15344b5a212d8a6d6da8d26",
      "max": 46841888,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a556607a3d8a41c2a54bcdd0545abd66",
      "value": 46841888
     }
    },
    "c739bf23bba54ed38fc97470f541c3c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48d014803cb34d1d97a556aa6a1ac39b",
       "IPY_MODEL_bda2d71f5d7e46babfeec1bcdf6abcbd",
       "IPY_MODEL_a7902ea5614649b28bc96388b1e56f27"
      ],
      "layout": "IPY_MODEL_e11aed3e9a7147c083398689c1931c2d"
     }
    },
    "d2d2976f1d4a47b78a68a3518b8db4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d88e2b9004504cf29da570d87b01af9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a78ee61e3f964c13b8b834f9f4dc6869",
      "placeholder": "​",
      "style": "IPY_MODEL_d2d2976f1d4a47b78a68a3518b8db4d6",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 153MB/s]"
     }
    },
    "d9374215bd5d45189e506eee10340f82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e11aed3e9a7147c083398689c1931c2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd853d84518f403dabe1b9529b39cd80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
