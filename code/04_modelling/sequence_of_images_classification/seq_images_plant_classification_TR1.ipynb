{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMePCPau-YjJ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade setuptools wheel\n",
    "%pip install torch-summary\n",
    "%pip install torchmetrics\n",
    "%pip install pytorch-gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:23.739437300Z",
     "start_time": "2023-05-28T09:44:23.721360669Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3263,
     "status": "ok",
     "timestamp": 1671181158309,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "3H6-g17F-KyY",
    "outputId": "0808cd1e-0e0f-4c20-bfe7-72707c056e15"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from scipy import stats\n",
    "import cv2 as cv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.nn.functional as F\n",
    "# for evaluation \n",
    "from torchmetrics import ConfusionMatrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "# from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "default_matplotlib_backend = matplotlib.get_backend()\n",
    "print('imported')\n",
    "print('default_matplotlib_backend: {}'.format(default_matplotlib_backend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:24.520649958Z",
     "start_time": "2023-05-28T09:44:24.509017693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('pytorch_utils'):\n",
    "    !git clone https://github.com/rishiswethan/pytorch_utils.git\n",
    "    print(\"waiting for pytorch_utils to be downloaded...\")\n",
    "    while not os.path.exists('pytorch_utils'):\n",
    "        time.sleep(1)\n",
    "    !cd pytorch_utils && git checkout v1.0.5 && cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-RuHNja-aMb"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:40.162742553Z",
     "start_time": "2023-05-28T09:44:40.155754368Z"
    }
   },
   "outputs": [],
   "source": [
    "RUN_MODE = ['DEV','LIVE'][1]\n",
    "SIMPLE_PATH = True  # Set this to false if you want to use custom paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:42.334739654Z",
     "start_time": "2023-05-28T09:44:42.265076407Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1671181158674,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "N0skx1mP-K2T",
    "outputId": "0b25acf7-b061-4b1e-c9a9-92fbf1264d45"
   },
   "outputs": [],
   "source": [
    "# Important, to have the same repartition of data between different machines\n",
    "np.random.seed(42)\n",
    "windows = (True if (os.name == 'nt') else False)\n",
    "\n",
    "def create_folder(new_path):\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "if SIMPLE_PATH or windows:\n",
    "    # Below path only applies for windows or if you want to use paths from root\n",
    "    root_path = os.getcwd() # path of the root project folder\n",
    "    extracted_data_path = root_path + \"\\\\datasets\\\\dataset_4a_n_crop\\\\\".replace(\"\\\\\", os.sep) # this one should exist before executing xD\n",
    "    weights_path =  root_path + \"\\\\weights\\\\\".replace(\"\\\\\", os.sep)\n",
    "    stats_path = root_path + \"\\\\stats\\\\\".replace(\"\\\\\", os.sep)\n",
    "    npy_data_path = root_path + \"\\\\npy_data\\\\\".replace(\"\\\\\", os.sep)\n",
    "    model_save_path = root_path + \"\\\\model\\\\\".replace(\"\\\\\", os.sep)\n",
    "    pretrained_models_folder = root_path + \"\\\\pretrained_models\\\\\".replace(\"\\\\\", os.sep)\n",
    "else:\n",
    "    # Below path only applies for linux, if you want to use custom paths\n",
    "    files_path_name = \"files_seq1\"\n",
    "    root_path = '/home/rsaric/Desktop/plant_classification/hyper_par1/' # path of the root project folder\n",
    "    extracted_data_path = '/media/medaghub_data/Rick/Classification_ex01/TRK-3a/dataset_4a_n_crop/' # this one should exist before executing xD\n",
    "    weights_path =  '/home/rsaric/Desktop/plant_classification/hyper_par1/weights/'\n",
    "    stats_path = '/home/rsaric/Desktop/plant_classification/hyper_par1/stats/'\n",
    "    npy_data_path = '/home/rsaric/Desktop/plant_classification/hyper_par1/npy_data/'\n",
    "    model_save_path = '/home/rsaric/Desktop/plant_classification/hyper_par1/model/model.pth'\n",
    "    pretrained_models_folder = '/home/rsaric/Desktop/plant_classification/hyper_par1/pretrained_models/'\n",
    "\n",
    "# creating the paths\n",
    "if not os.path.exists(npy_data_path):\n",
    "    os.makedirs(npy_data_path)\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "if not os.path.exists(stats_path):\n",
    "    os.makedirs(stats_path)\n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_save_path)):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "train_valid_test_split_json_name = 'NEW25split.json'\n",
    "\n",
    "# Define dataset paths based on the chosen JSON file\n",
    "if train_valid_test_split_json_name == 'NEW8split.json':\n",
    "    dataset_path = \"/mnt/medaghub-ws/Rick/CR_datasets/dataset_3n_a_auto_cropped/\"\n",
    "elif train_valid_test_split_json_name == 'NEW25split.json':\n",
    "    dataset_path = \"/mnt/medaghub-ws/Rick/CR_datasets/dataset_4a_n_auto_cropped/\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown JSON file: {train_valid_test_split_json_name}\")\n",
    "\n",
    "# Update extracted_data_path accordingly\n",
    "extracted_data_path = dataset_path\n",
    "\n",
    "# getting the list of the classes\n",
    "class_list = os.listdir(extracted_data_path)\n",
    "class_list.sort()\n",
    "print('Number of classes: {}'.format(len(class_list)))\n",
    "\n",
    "# working device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Selected device: {}'.format(device))\n",
    "if device.type != 'cpu':\n",
    "    print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# other cosntants\n",
    "# you can modify from here: and only modify whats possible to modify (for backbones and optimizers)\n",
    "BATCH_SIZE = 8 if RUN_MODE == 'DEV' else 16\n",
    "DAY_TO_USE = 7\n",
    "IMAGES_IN_A_DAY = 2\n",
    "SEQ_LEN = DAY_TO_USE * IMAGES_IN_A_DAY\n",
    "EPOCHS = 60\n",
    "IMG_SIZE = (350, 350) # base size! if you want to change it you have to do it by T.Resize(*target_size). .npy data already on (350, 350)\n",
    "RESIZE_SIZE = (128, 128)\n",
    "NUM_CHANNELS = 3\n",
    "BACKBONE = 'EfficientNetB2'\n",
    "OPTIMIZER = 'Adam'\n",
    "\n",
    "available_backbones = [\n",
    "    'AlexNet', 'ResNet18', 'ResNet34', 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2',\n",
    "    'EfficientNetV2_S', 'ConvNext_T', 'MobileNet_V3_Small', 'MobileNet_V3_Large',\n",
    "    'ViT_B_16'\n",
    "]\n",
    "\n",
    "avaible_optimizers = ['SGD', 'Adam', 'AdamW', 'RMSprop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12NNSwRK-dP7"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:45.006210817Z",
     "start_time": "2023-05-28T09:44:44.902718938Z"
    },
    "executionInfo": {
     "elapsed": 53375,
     "status": "ok",
     "timestamp": 1671181212044,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "erAlgBp9-K7j"
   },
   "outputs": [],
   "source": [
    "#specify which subdataset to use:\n",
    "DATASE_GROUP_IDX = None #could be: None, 0, 1, 2\n",
    "\n",
    "#availiable subdatasets:\n",
    "indep1 = [18, 9, 26, 29, 34]\n",
    "\n",
    "dataset_groups = [\n",
    "    [1, 7, 10, 14, 11, 32, 13, 4, 8, 17, 20, 25, 28, 31, 36],\n",
    "    [0, 15, 21, 22, 23, 5]  + indep1,\n",
    "    [37, 33, 16, 2, 3, 6, 12, 19, 24, 27, 30, 35],\n",
    "]\n",
    "\n",
    "# some stats about the data\n",
    "image_files = []\n",
    "targets = []\n",
    "\n",
    "# dict helps to go from class_name to class_index\n",
    "class_dict = dict([(j, i) for i, j in enumerate(class_list)])\n",
    "\n",
    "# loading all image_paths (IN A SORTED ORDER, this is really important to avoid any weird exceptions)\n",
    "for class_name in class_dict.keys():\n",
    "    if class_name.startswith('.'):\n",
    "        continue\n",
    "    repetitions_list = os.listdir(extracted_data_path + class_name)\n",
    "    repetitions_list.sort()\n",
    "    for repetition in repetitions_list:\n",
    "        if repetition.startswith('.'):\n",
    "            continue\n",
    "        image_list = os.listdir(extracted_data_path + class_name + os.sep + repetition)\n",
    "        image_list.sort()\n",
    "        image_files.extend(\n",
    "            [extracted_data_path + class_name + os.sep + repetition + os.sep + img for img in image_list]\n",
    "        )\n",
    "        targets.extend([class_dict[class_name]] * len(image_list))\n",
    "\n",
    "targets = np.array(targets)\n",
    "\n",
    "if DATASE_GROUP_IDX is not None:\n",
    "    dataset_groups = [sorted(el) for el in dataset_groups]\n",
    "    assert 0 <= DATASE_GROUP_IDX <len(dataset_groups), '...'\n",
    "\n",
    "    mapper_allcls_to_subcls = {j:i for i,j in enumerate(dataset_groups[DATASE_GROUP_IDX])}\n",
    "    class_dict = {class_list[j]:i for i,j in enumerate(dataset_groups[DATASE_GROUP_IDX])}\n",
    "    class_list = [class_list[j] for i, j in enumerate(dataset_groups[DATASE_GROUP_IDX])]\n",
    "\n",
    "    new_targets = []\n",
    "    new_image_files = []\n",
    "    for t,im in zip(targets, image_files):\n",
    "        if t in mapper_allcls_to_subcls:\n",
    "            new_targets.append(mapper_allcls_to_subcls[t])\n",
    "            new_image_files.append(im)\n",
    "    new_targets = np.array(new_targets)\n",
    "    targets = new_targets\n",
    "    image_files = new_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:45.323757712Z",
     "start_time": "2023-05-28T09:44:45.306679051Z"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1671181212046,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "V38N8QwdKmBQ"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def _fix_images_paths(images_paths, targets):\n",
    "    images_paths = deepcopy(images_paths)\n",
    "    targets = deepcopy(targets)\n",
    "    while os.path.basename(images_paths[0]).split('_')[1:4] != os.path.basename(image_files[1]).split('_')[1:4]:\n",
    "        images_paths = images_paths[1:]\n",
    "        targets = targets[1:]\n",
    "    while os.path.basename(images_paths[-1]).split('_')[1:4] != os.path.basename(image_files[-2]).split('_')[1:4]:\n",
    "        images_paths = images_paths[:-1]\n",
    "        targets = targets[:-1]\n",
    "    return images_paths, targets\n",
    "\n",
    "def prepare_dataset(image_files, targets, train_valid_test_split, fix_len=10):\n",
    "    images_paths, targets_cls = _fix_images_paths(image_files, targets)\n",
    "\n",
    "    data_as_dict = {}\n",
    "    for el, _trg in zip(images_paths, targets_cls):\n",
    "        _cls, _rep_name = el.split(os.sep)[-3:-1]\n",
    "        k = (_cls, _rep_name)\n",
    "        if k not in data_as_dict:\n",
    "            data_as_dict[k] = []\n",
    "        data_as_dict[k].append((el, _trg))\n",
    "\n",
    "    # Add more images to the dataset to make it divisible by fix_len\n",
    "    for k in data_as_dict:\n",
    "        while len(data_as_dict[k]) % fix_len != 0:\n",
    "            data_as_dict[k].append(data_as_dict[k][-1])\n",
    "    \n",
    "    datas = {}\n",
    "    for k in train_valid_test_split:\n",
    "        for rep_name in train_valid_test_split[k]:\n",
    "            if (k, rep_name) not in data_as_dict:\n",
    "                continue\n",
    "            data_type = train_valid_test_split[k][rep_name]\n",
    "            if data_type not in datas:\n",
    "                datas[data_type] = []\n",
    "            files_in_rep = len(data_as_dict[(k, rep_name)])\n",
    "            for start_idx in range(0,files_in_rep-fix_len+1,2):\n",
    "                seq = data_as_dict[(k, rep_name)][start_idx:start_idx+fix_len]\n",
    "                assert len(seq) == fix_len, '...'\n",
    "                datas[data_type].append(seq)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:47.444382890Z",
     "start_time": "2023-05-28T09:44:47.379354855Z"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1671181212048,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "oHXGVaw1MdXp"
   },
   "outputs": [],
   "source": [
    "# train_valid_test_split_json_name = 'train_valid_test_split_DS1a.json'\n",
    "train_valid_test_split_json_name = 'NEW25split.json' \n",
    "with open(os.path.join(train_valid_test_split_json_name), 'r') as f:\n",
    "    train_valid_test_split = json.load(f)\n",
    "datas = prepare_dataset(image_files, targets, train_valid_test_split, fix_len=max(SEQ_LEN, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:47.557290515Z",
     "start_time": "2023-05-28T09:44:47.420841948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1671181212049,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "WaQn5fkhMfng",
    "outputId": "9cb311a0-1636-4836-b047-1633d4dc0fc1"
   },
   "outputs": [],
   "source": [
    "USE_AUGMENTATION = False\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "def zoom_at(img, zoom=1, angle=0, coord=None):\n",
    "    cy, cx = [ i/2 for i in img.shape[:-1] ] if coord is None else coord[::-1]\n",
    "    rot_mat = cv.getRotationMatrix2D((cx,cy), angle, zoom)\n",
    "    result = cv.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "class ClassificationPlantSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, use_augmentation=False):\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.data = data\n",
    "\n",
    "        if self.use_augmentation:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(RESIZE_SIZE[0], RESIZE_SIZE[1]),\n",
    "                A.Rotate(limit=180, p=0.7),\n",
    "                A.Flip(p=0.9),\n",
    "                A.ShiftScaleRotate(shift_limit=0.2, scale_limit=(0.2, 0.5)),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(RESIZE_SIZE[0], RESIZE_SIZE[1]),\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq, classes = list(zip(*self.data[index]))\n",
    "        assert len(set(classes)) == 1, 'wrong seq clses'\n",
    "        seq_cls = classes[0]\n",
    "\n",
    "        images = []\n",
    "        for im_path in seq:\n",
    "            img = cv.imread(im_path)\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            img = cv.resize(img, IMG_SIZE)\n",
    "            img = self.transform(image=img)['image']\n",
    "            img = img.astype(np.float32) / 255.\n",
    "            img = torch.from_numpy(img.transpose((2, 0, 1)))\n",
    "            images.append(img)\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        images = torch.permute(images, (1, 0, 2, 3))\n",
    "        return images, seq_cls\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# loading data\n",
    "train_dataset = ClassificationPlantSequenceDataset(datas['train'], use_augmentation=USE_AUGMENTATION)\n",
    "val_dataset = ClassificationPlantSequenceDataset(datas['valid'])\n",
    "test_dataset = ClassificationPlantSequenceDataset(datas['test'])\n",
    "\n",
    "# # combine val and test\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset, test_dataset])\n",
    "test_dataset = val_dataset\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "print('Data loaders created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:54.006475431Z",
     "start_time": "2023-05-28T09:44:49.534705032Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 159854,
     "status": "ok",
     "timestamp": 1671181371883,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "QtLcvfmtMw1u",
    "outputId": "05d13107-9fb6-47fc-c4a6-b90be31a511c"
   },
   "outputs": [],
   "source": [
    "dataset = ClassificationPlantSequenceDataset(datas['train'], use_augmentation=USE_AUGMENTATION)\n",
    "idxs_in_batch = [0,1,2]\n",
    "sample_size = 10\n",
    "for idx_in_batch in idxs_in_batch:\n",
    "    img_idx = np.random.randint(0, len(dataset), size=sample_size)\n",
    "    sample = [dataset[i] for i in img_idx]\n",
    "    imgs = sample[idx_in_batch][0].cpu()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(sample_size):\n",
    "        img = imgs[:,i,:,:].permute(1, 2, 0)\n",
    "        plt.subplot(1, sample_size, i + 1)\n",
    "        plt.imshow((img * 255).type(torch.uint8))\n",
    "        plt.title('Class: {}'.format(class_list[sample[idx_in_batch][1]]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7mGbOA21QiJ"
   },
   "source": [
    "#### Dataset class and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxKuuukRC9f3"
   },
   "source": [
    "### Data sample (Run this only if you want to see an example of the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gGak8ErYs48"
   },
   "source": [
    "### LR Scheduler and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:54.019251671Z",
     "start_time": "2023-05-28T09:44:54.005716743Z"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1671181371886,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "2Dmhcs-AYtFY"
   },
   "outputs": [],
   "source": [
    "class PolyScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, base_lr, max_steps, warmup_steps, last_epoch=-1):\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_lr_init = 0.0001\n",
    "        self.max_steps: int = max_steps\n",
    "        self.warmup_steps: int = warmup_steps\n",
    "        self.power = 2\n",
    "        super(PolyScheduler, self).__init__(optimizer, -1, False)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_warmup_lr(self):\n",
    "        alpha = float(self.last_epoch) / float(self.warmup_steps)\n",
    "        return [self.base_lr * alpha for _ in self.optimizer.param_groups]\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch == -1:\n",
    "            return [self.warmup_lr_init for _ in self.optimizer.param_groups]\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            return self.get_warmup_lr()\n",
    "        else:\n",
    "            alpha = pow(\n",
    "                1\n",
    "                - float(self.last_epoch - self.warmup_steps)\n",
    "                / float(self.max_steps - self.warmup_steps),\n",
    "                self.power,\n",
    "            )\n",
    "            return [self.base_lr * alpha for _ in self.optimizer.param_groups]\n",
    "\n",
    "def plot_model_stats(model_name, train_loss, train_acc, test_loss, test_acc):\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, label = 'Train loss')\n",
    "    plt.plot(np.arange(len(test_loss)), test_loss, label = 'Val loss')\n",
    "    plt.title('Model: {} - Validation loss: {:.4f}'.format(model_name, test_loss[-1]))\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.arange(len(train_acc)), np.array(train_acc) * 100, color='green', label = 'Train accuracy')\n",
    "    plt.plot(np.arange(len(test_acc)), np.array(test_acc) * 100, color='red', label = 'Val accuracy')\n",
    "    plt.title('Model: {} - Validation accuracy: {:.2f} %'.format(model_name, test_acc[-1] * 100))\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def load_model_weights(BACKBONE=BACKBONE, OPTIMIZER=OPTIMIZER):\n",
    "    # this function loads the whole model with weights\n",
    "    pth = os.path.join(weights_path, 'backbone_{}_{}.pth'.format(BACKBONE, OPTIMIZER))\n",
    "    assert os.path.exists(pth), 'Configuration not found'\n",
    "    model = torch.load(pth).to(device)\n",
    "    print('Backbone weights loaded: {}'.format(BACKBONE))\n",
    "    return model\n",
    "\n",
    "def load_model_stats(BACKBONE=BACKBONE, OPTIMIZER=OPTIMIZER):\n",
    "    # this function loads training stats (train/val loss and acc)\n",
    "    stats_file = os.path.join(stats_path, 'stats_{}_{}.pkl'.format(BACKBONE, OPTIMIZER))\n",
    "    assert os.path.exists(stats_file), 'Configuration not found'\n",
    "    with open(stats_file, 'rb') as stats:\n",
    "        stats = pickle.load(stats)\n",
    "    print('Train/Validation stats loaded for the model: {}'.format(BACKBONE))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-jWW_2O_Yoy"
   },
   "source": [
    "## Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:56.493167768Z",
     "start_time": "2023-05-28T09:44:56.487220495Z"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1671181371888,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "SP6pjJzJzmH5"
   },
   "outputs": [],
   "source": [
    "def _handlezero_division_np(a,b):\n",
    "    # initialize output tensor with desired value\n",
    "    c = np.zeros_like(a)\n",
    "    mask = (b != 0)\n",
    "    # finally perform division\n",
    "    c[mask] = a[mask] / b[mask]\n",
    "    return c\n",
    "\n",
    "def mathews_correlation_coefficient_np(tp, fp, fn, tn):\n",
    "    tp = tp.sum().astype(np.float64)\n",
    "    tn = tn.sum().astype(np.float64)\n",
    "    fp = fp.sum().astype(np.float64)\n",
    "    fn = fn.sum().astype(np.float64)\n",
    "    _numerator = (tp*tn - fp*fn)\n",
    "    _denomerator = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    x = _numerator / (_denomerator + 1e-11)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:44:58.783493144Z",
     "start_time": "2023-05-28T09:44:58.778048955Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_utils.callbacks as pt_callbacks\n",
    "def get_callbacks(\n",
    "        optimiser,\n",
    "        result,\n",
    "        model,\n",
    "        defined_callbacks=None,\n",
    "        continue_training=False,\n",
    "        other_stats=None\n",
    "):\n",
    "\n",
    "    if defined_callbacks is None:\n",
    "        defined_callbacks = {\n",
    "            'val': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                          model_save_path=model_save_path + 'model.pth',\n",
    "                                          training_stats_path=model_save_path + 'training_stats_val',\n",
    "                                          continue_training=continue_training),\n",
    "\n",
    "            'train': pt_callbacks.Callbacks(optimizer=optimiser,\n",
    "                                            training_stats_path=model_save_path + 'training_stats_train',\n",
    "                                            continue_training=continue_training)\n",
    "        }\n",
    "\n",
    "    defined_callbacks['val'].reduce_lr_on_plateau(\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        indicator_text=\"Val LR scheduler: \"\n",
    "    )\n",
    "    defined_callbacks['val'].model_checkpoint(\n",
    "        model=model,\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        indicator_text=\"Val checkpoint: \"\n",
    "    )\n",
    "    stop_flag = defined_callbacks['val'].early_stopping(\n",
    "        monitor_value=result[\"val_acc\"],\n",
    "        mode='max',\n",
    "        patience=25,\n",
    "        indicator_text=\"Early stopping: \"\n",
    "    )\n",
    "    defined_callbacks['val'].clear_memory()\n",
    "    print(\"_________\")\n",
    "    return defined_callbacks, stop_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training loop and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:45:00.905183280Z",
     "start_time": "2023-05-28T09:45:00.838690120Z"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1671181371890,
     "user": {
      "displayName": "Test112a te53",
      "userId": "00279439771611025277"
     },
     "user_tz": -120
    },
    "id": "B7Y8z2bk-RiV"
   },
   "outputs": [],
   "source": [
    "import pytorch_utils.training_utils as pt_train\n",
    "\n",
    "def train_loop(\n",
    "        model,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        model_save_folder=model_save_path,\n",
    "        initial_lr=0.001,\n",
    "        weight_decay=None,\n",
    "        running_hyperopt=False,\n",
    "        verbose=False,\n",
    "        continue_training=False,\n",
    "):\n",
    "    def get_result_list(history, metric):\n",
    "        return [history[i][metric] for i in range(len(history))]\n",
    "\n",
    "    # prep the model save path\n",
    "    shutil.rmtree(model_save_folder, ignore_errors=True)\n",
    "    os.makedirs(model_save_folder, exist_ok=True)\n",
    "\n",
    "    # Train the model using torch\n",
    "    history = pt_train.fit(\n",
    "        epochs=epochs,\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay,\n",
    "        model=model,\n",
    "        continue_training=continue_training,\n",
    "        callbacks_function=get_callbacks,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        opt_func=optimizer,\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    # load the best model from checkpoint\n",
    "    model = torch.load(model_save_path + \"model.pth\")\n",
    "    train_loss_history = get_result_list(history, \"train_loss\")\n",
    "    train_acc_history = get_result_list(history, \"train_acc\")\n",
    "    val_loss_history = get_result_list(history, \"val_loss\")\n",
    "    val_acc_history = get_result_list(history, \"val_acc\")\n",
    "    return model, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, verbose=True, eps=1e-10):\n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    confusion_matrix = ConfusionMatrix(num_classes=len(class_list))\n",
    "    eval_preds = list()\n",
    "    eval_targs = list()\n",
    "\n",
    "    # computing predictions and confusion matrix\n",
    "    for i, (images, targets) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "        images, targets = images.to(device, dtype=torch.float), torch.Tensor(targets).to(device)\n",
    "        outputs = torch.nn.functional.log_softmax(model(images), dim=1)\n",
    "        preds = torch.argsort(outputs, dim=1, descending=True)[:, :3]\n",
    "        eval_preds.extend(preds[:, 0].cpu().numpy())\n",
    "        eval_targs.extend(targets.cpu().numpy())\n",
    "\n",
    "    # computing main metrics (acc, precisio, recall and f1 score)\n",
    "    matrix = confusion_matrix(torch.tensor(eval_preds), torch.tensor(eval_targs))\n",
    "    accuracy = matrix.trace() / (matrix.sum()+eps)\n",
    "    # loss = F.cross_entropy(torch.tensor(eval_preds), torch.tensor(eval_targs))\n",
    "    precision = np.array([matrix[i, i] / (matrix.sum(axis=0)[i]+eps) for i in range(len(class_list))])\n",
    "    recall = np.array([matrix[i, i] / (matrix.sum(axis=1)[i]+eps) for i in range(len(class_list))])\n",
    "    f1_score = 2 * precision * recall / (precision + recall+eps)\n",
    "\n",
    "    # computing false positive rate, false negative rate, false discovery rate, false omission rate\n",
    "    fp_rate = np.zeros(len(class_list))\n",
    "    for idx in range(len(class_list)):\n",
    "        tn = matrix.trace() - matrix[idx, idx]\n",
    "        fp = np.sum([matrix[j, idx] for j in range(len(class_list)) if j != idx])\n",
    "        fp_rate[idx] = fp / (fp + tn+eps)\n",
    "\n",
    "    fn_rate = 1 - recall\n",
    "    fd_rate = 1 - precision\n",
    "    specificity = 1 - fp_rate\n",
    "\n",
    "    fo_rate = np.zeros(len(class_list))\n",
    "    for idx in range(len(class_list)):\n",
    "        n = np.sum(np.array(eval_targs) != idx)\n",
    "        fn = np.sum([matrix.sum(axis=0)[j] - matrix[j, j] for j in range(len(class_list)) if j != idx])\n",
    "        fo_rate[idx] = fn / (n+eps)\n",
    "\n",
    "    missclassification_rate = 1 - accuracy\n",
    "    npv = 1 - fo_rate\n",
    "\n",
    "    mcc_per_class = []\n",
    "    for idx in range(len(class_list)):\n",
    "        tp = matrix[idx, idx].cpu().numpy()\n",
    "        tn = (matrix.trace() - matrix[idx, idx]).cpu().numpy()\n",
    "        fp = np.sum([matrix[j, idx] for j in range(len(class_list)) if j != idx])\n",
    "        fn = np.sum([matrix.sum(axis=0)[j] - matrix[j, j] for j in range(len(class_list)) if j != idx])\n",
    "        _mcc = mathews_correlation_coefficient_np(tp, fp, fn, tn)\n",
    "        mcc_per_class.append(_mcc)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Accuracy: {:.3f}%'.format(accuracy * 100))\n",
    "        # print('Loss: {:.3f}'.format(loss))\n",
    "        print('Average precision: {:.3f}'.format(precision.mean()))\n",
    "        print('Average recall: {:.3f}'.format(recall.mean()))\n",
    "        print('Average F1 score: {:.3f}'.format(f1_score.mean()))\n",
    "        print('Average specificity: {:.3f}'.format(specificity.mean()))\n",
    "        print('Average false positive rate: {:3f}'.format(fp_rate.mean()))\n",
    "        print('Average false negative rate: {:3f}'.format(fn_rate.mean()))\n",
    "        print('Average false discovery rate: {:.3f}'.format(fd_rate.mean()))\n",
    "        print('Average false omission rate: {:.3f}'.format(fo_rate.mean()))\n",
    "        print('Missclassification rate: {:.2f}%'.format(missclassification_rate * 100))\n",
    "        print('Mathews Correlation Coefficient: {:.2f}'.format(np.mean(mcc_per_class)))\n",
    "        print('--------------------------------------------')\n",
    "        print('Results by class :')\n",
    "        print('--------------------------------------------')\n",
    "        print('{:<15}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}{:<12}'.format('', 'Precision', 'Recall', 'F1 score', 'Specificity', 'FPR', 'FNR', 'FDR', 'FOR', 'NPV', 'MCC'))\n",
    "        for idx, class_name in enumerate(class_list):\n",
    "            print('{:<15}{:<12.2f}{:<12.2f}{:<12.2f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}{:<12.3f}'.format(\n",
    "                class_name, precision[idx], recall[idx], f1_score[idx], specificity[idx], fp_rate[idx], fn_rate[idx], fd_rate[idx], fo_rate[idx], npv[idx], mcc_per_class[idx]\n",
    "            ))\n",
    "        print('--------------------------------------------')\n",
    "        print()\n",
    "\n",
    "        # ploting confusion matrix\n",
    "        matrix_df = pd.DataFrame(matrix.numpy(), index=class_list, columns=class_list)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sn.heatmap(matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "    return accuracy, precision, recall, f1_score, matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:45:02.790338493Z",
     "start_time": "2023-05-28T09:45:02.776264278Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(\n",
    "        outputs: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom accuracy function to override the default one in pt_train\n",
    "    \"\"\"\n",
    "\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class CustomModelBase(pt_train.CustomModelBase):\n",
    "    \"\"\"\n",
    "    ModelBase override for training and validation steps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None):\n",
    "        super(CustomModelBase, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.accuracy_function = accuracy\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels, weight=self.class_weights)  # Calculate loss with class weights\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels, weight=self.class_weights)  # Calculate loss with class weights\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T09:53:44.896521190Z",
     "start_time": "2023-05-28T09:53:44.749869024Z"
    }
   },
   "outputs": [],
   "source": [
    "class R3D_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(R3D_18, self).__init__()\n",
    "        self.model = models.video.r3d_18(weights=models.video.R3D_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# mc3_18\n",
    "# 4M parameters\n",
    "class MC3_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MC3_18, self).__init__()\n",
    "        self.model = models.video.mc3_18(weights=models.video.MC3_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# r2plus1d_18\n",
    "# 4M parameters\n",
    "class r2plus1d_18(CustomModelBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(r2plus1d_18, self).__init__()\n",
    "        self.model = models.video.r2plus1d_18(weights=models.video.R2Plus1D_18_Weights.DEFAULT)\n",
    "        self.linear = nn.Linear(self.model.fc.out_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4M parameters\n",
    "# no init weights\n",
    "class Eff_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=False)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "        h0 = torch.zeros(self.num_layers, embeddings.shape[1], self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, embeddings.shape[1], self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.gru = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff2_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(Eff2_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=0.5)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(MNV3S_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(MNV3S_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(MNV3S_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class MNV3S_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(MNV3S_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.classifier[-1].out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R18_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R18_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R18_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R18_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R18_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R34_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R34_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R34_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R34_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R34_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "# ---\n",
    "\n",
    "# 4M parameters\n",
    "class R50_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R50_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R50_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R50_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R50_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R50_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "#-----------------------------#\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class R101_LSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R101_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_GRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2):\n",
    "        super(R101_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_BiLSTM(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R101_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.lstm = nn.LSTM(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class R101_BiGRU(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=200, num_layers=2, bidirectional=True):\n",
    "        super(R101_BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.gru = nn.GRU(self.model.fc.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vivit_modules import ViViT\n",
    "# ViViT Small\n",
    "ViViT_Small = ViViT(heads=3, depth=4, dim=192)\n",
    "\n",
    "# ViViT Large\n",
    "ViViT_Large = ViViT(heads=12, depth=24, dim=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:15.789857470Z",
     "start_time": "2023-05-28T10:30:15.709135236Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DROPOUT = 0.0\n",
    "HIDDEN_SIZE = 256\n",
    "PRETRAINED_NUM_CLASSES = len(class_list)  # default was len(class_list)\n",
    "#--------------------------------\n",
    "# EfficientNetB1 pretrained\n",
    "#--------------------------------\n",
    "\n",
    "class EfficientNetB1(CustomModelBase):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetB1, self).__init__()\n",
    "        self.model = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff1_pretrained(EfficientNetB1):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}eff_b1.pth\"\n",
    "    ):\n",
    "        super(Eff1_pretrained, self).__init__()\n",
    "        # self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_b1().classifier[-1].in_features\n",
    "\n",
    "        # self.out_features = self.model.fc.out_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff1_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff1_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Eff1_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff1_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class Eff1_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff1_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff1_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff1_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff1_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "#--------------------------------\n",
    "# EfficientNetB2 pretrained\n",
    "#--------------------------------\n",
    "\n",
    "class EfficientNetB2(CustomModelBase):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetB2, self).__init__()\n",
    "        self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff2_pretrained(EfficientNetB2):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}eff_b2.pth\"\n",
    "    ):\n",
    "        super(Eff2_pretrained, self).__init__()\n",
    "        # self.model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_b2().classifier[-1].in_features\n",
    "\n",
    "        # self.out_features = self.model.fc.out_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Eff2_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff2_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Eff2_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(Eff2_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 4M parameters\n",
    "class Eff2_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True):\n",
    "        super(Eff2_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = Eff2_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN * self.num_directions, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*self.num_directions, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "#---------------------------------\n",
    "# EfficientNetV2\n",
    "#---------------------------------\n",
    "\n",
    "class EfficientNetV2_S(nn.Module):\n",
    "    def __init__(self, num_classes=PRETRAINED_NUM_CLASSES):\n",
    "        super(EfficientNetV2_S, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EffV2_S_pretrained(EfficientNetV2_S):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the .pth file and remove the last layer, so that the model can be used as a feature extractor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=None,\n",
    "            pretrained_model_path=f\"{pretrained_models_folder}effv2_s.pth\"\n",
    "    ):\n",
    "        super(EffV2_S_pretrained, self).__init__()\n",
    "\n",
    "        loaded_model = torch.load(pretrained_model_path)\n",
    "        self.load_state_dict(loaded_model.state_dict())\n",
    "\n",
    "        self.out_features = models.efficientnet_v2_s().classifier[-1].in_features\n",
    "\n",
    "        self.model.classifier = nn.Identity()  # Remove last layer. Final layer in not useful when using this model as feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EffV2_S_GRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_GRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_LSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_LSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_BiGRU_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_BiGRU_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.gru = nn.GRU(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * 2 * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        out, h = self.gru(embeddings, h)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EffV2_S_BiLSTM_pretrained(CustomModelBase):\n",
    "    def __init__(self, num_classes, hidden_size=HIDDEN_SIZE, num_layers=2):\n",
    "        super(EffV2_S_BiLSTM_pretrained, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.model = EffV2_S_pretrained()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.model.out_features, self.hidden_size, self.num_layers, batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(self.hidden_size * 2 * SEQ_LEN, num_classes*2, bias=True)\n",
    "        self.linear2 = nn.Linear(num_classes*2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in range(SEQ_LEN):\n",
    "            emb = self.model(x[:,:,idx])\n",
    "            embeddings.append(emb[:,None])\n",
    "        embeddings = torch.concat(embeddings, 1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(embeddings, (h0.detach(), c0.detach()))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:18.055838367Z",
     "start_time": "2023-05-28T10:30:18.043302546Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_by_name(name):\n",
    "    model_directory = {\n",
    "        \"R3D_18\": R3D_18,\n",
    "        \"MC3_18\": MC3_18,\n",
    "        \"r2plus1d_18\": r2plus1d_18,\n",
    "        \"Eff_LSTM\": Eff_LSTM,\n",
    "        \"Eff_GRU\": Eff_GRU,\n",
    "        \"Eff_BiGRU\": Eff_BiGRU,\n",
    "        \"Eff_BiLSTM\": Eff_BiLSTM,\n",
    "        \"Eff2_LSTM\": Eff2_LSTM,\n",
    "        \"Eff2_GRU\": Eff2_GRU,\n",
    "        \"Eff2_BiLSTM\": Eff2_BiLSTM,\n",
    "        \"Eff2_BiGRU\": Eff2_BiGRU,\n",
    "        \"MNV3S_LSTM\": MNV3S_LSTM,\n",
    "        \"MNV3S_GRU\": MNV3S_GRU,\n",
    "        \"MNV3S_BiLSTM\": MNV3S_BiLSTM,\n",
    "        \"MNV3S_BiGRU\": MNV3S_BiGRU,\n",
    "        \"R18_LSTM\": R18_LSTM,\n",
    "        \"R18_GRU\": R18_GRU,\n",
    "        \"R18_BiLSTM\": R18_BiLSTM,\n",
    "        \"R18_BiGRU\": R18_BiGRU,\n",
    "        \"R34_LSTM\": R34_LSTM,\n",
    "        \"R34_GRU\": R34_GRU,\n",
    "        \"R34_BiLSTM\": R34_BiLSTM,\n",
    "        \"R34_BiGRU\": R34_BiGRU,\n",
    "        \"R50_LSTM\": R50_LSTM,\n",
    "        \"R50_GRU\": R50_GRU,\n",
    "        \"R50_BiLSTM\": R50_BiLSTM,\n",
    "        \"R50_BiGRU\": R50_BiGRU,\n",
    "        \"R101_LSTM\": R101_LSTM,\n",
    "        \"R101_GRU\": R101_GRU,\n",
    "        \"R101_BiLSTM\": R101_BiLSTM,\n",
    "        \"R101_BiGRU\": R101_BiGRU,\n",
    "        \"Eff1_GRU_pretrained\": Eff1_GRU_pretrained,\n",
    "        \"Eff1_LSTM_pretrained\": Eff1_LSTM_pretrained,\n",
    "        \"Eff1_BiGRU_pretrained\": Eff1_BiGRU_pretrained,\n",
    "        \"Eff1_BiLSTM_pretrained\": Eff1_BiLSTM_pretrained,\n",
    "        \"Eff2_GRU_pretrained\": Eff2_GRU_pretrained,\n",
    "        \"Eff2_LSTM_pretrained\": Eff2_LSTM_pretrained,\n",
    "        \"Eff2_BiGRU_pretrained\": Eff2_BiGRU_pretrained,\n",
    "        \"Eff2_BiLSTM_pretrained\": Eff2_BiLSTM_pretrained,\n",
    "        \"EffV2_S_GRU_pretrained\": EffV2_S_GRU_pretrained,\n",
    "        \"EffV2_S_LSTM_pretrained\": EffV2_S_LSTM_pretrained,\n",
    "        \"EffV2_S_BiGRU_pretrained\": EffV2_S_BiGRU_pretrained,\n",
    "        \"EffV2_S_BiLSTM_pretrained\": EffV2_S_BiLSTM_pretrained,\n",
    "        \"ViViT_Small\": ViViT_Small,\n",
    "        \"ViViT_Large\": ViViT_Large\n",
    "    }\n",
    "    return model_directory[name]\n",
    "\n",
    "def get_optimizer_by_name(optim):\n",
    "    if optim == 'SGD':\n",
    "        optimizer = torch.optim.SGD #(model.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-6)\n",
    "    elif optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam #(model.parameters(), lr=0.02, weight_decay=1e-6)\n",
    "    elif optim == 'RMSProp':\n",
    "        optimizer = torch.optim.RMSprop #(model.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-6)\n",
    "    elif optim == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW #(model.parameters(), lr=0.02, weight_decay=1e-6)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:20.645062438Z",
     "start_time": "2023-05-28T10:30:20.632931325Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, space_eval, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "def train_model(kwargs):\n",
    "\n",
    "    print(kwargs)\n",
    "    epochs = kwargs.get(\"epochs\", EPOCHS)\n",
    "\n",
    "    # get model by name\n",
    "    model_name = kwargs.get(\"model\")\n",
    "    model_cls = get_model_by_name(model_name)\n",
    "    model = model_cls(num_classes=len(class_list)).to(device)\n",
    "\n",
    "    # get optimizer\n",
    "    optim_args = kwargs.get(\"optim\")\n",
    "    print(optim_args[\"params\"])\n",
    "    optimizer_cls = get_optimizer_by_name(optim_args.get(\"name\"))\n",
    "\n",
    "    model, train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_loop(\n",
    "        model,\n",
    "        optimizer_cls,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        initial_lr=optim_args[\"params\"][\"lr\"],\n",
    "        weight_decay=optim_args[\"params\"][\"weight_decay\"],\n",
    "        verbose=True,\n",
    "        running_hyperopt=True,\n",
    "        continue_training=False\n",
    "    )\n",
    "\n",
    "    return model, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "\n",
    "def train_model_hyperopt(kwargs):\n",
    "\n",
    "    model, train_loss_history, train_acc_history, val_loss_history, val_acc_history  = train_model(kwargs)\n",
    "\n",
    "    return {\"loss\": np.mean(val_loss_history), \"status\": STATUS_OK}\n",
    "\n",
    "def unpack_values(trial):\n",
    "    vals = trial[\"misc\"][\"vals\"]\n",
    "    # unpack the one-element lists to values\n",
    "    # and skip over the 0-element lists\n",
    "    rval = {}\n",
    "    for k, v in list(vals.items()):\n",
    "        if v:\n",
    "            rval[k] = v[0]\n",
    "    return rval\n",
    "\n",
    "def export_hyperopt_log(trials):\n",
    "    result_list = []\n",
    "    for trial in trials.trials:\n",
    "        trial_result = space_eval(search_space, unpack_values(trial))\n",
    "        trial_result[\"val_loss\"] = trial['result']['loss']\n",
    "        result_list.append(trial_result)\n",
    "\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    df_result = pd.concat([df_result.drop(\"optim\", axis=1), pd.json_normalize(df_result.optim)], axis=1)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"../data/log\"\n",
    "    output_path = os.path.join(output_dir, f\"hyperopt_result_{ts}.csv\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"Exported hyperopt log to {output_path}\")\n",
    "    return df_result\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"epochs\": scope.int(hp.choice(\"epochs\", [7, 10, 13, 15, 17, 20, 22, 25, 27, 30, 33, 35])),\n",
    "    \"model\":  hp.choice(\"model_name\", [\n",
    "         \"R3D_18\",\n",
    "         \"MC3_18\",\n",
    "         \"r2plus1d_18\",\n",
    "        # \"Eff_LSTM\",\n",
    "        \"Eff_GRU\",\n",
    "        # \"Eff_BiGRU\",\n",
    "        # \"Eff_BiLSTM\",\n",
    "        #\"Eff2_LSTM\",\n",
    "        \"Eff2_GRU\",\n",
    "        #\"Eff2_BiLSTM\",\n",
    "        #\"Eff2_BiGRU\",\n",
    "        # \"MNV3S_LSTM\",\n",
    "        \"MNV3S_GRU\",\n",
    "        # \"MNV3S_BiLSTM\",\n",
    "        # \"MNV3S_BiGRU\",\n",
    "        # \"R18_LSTM\",\n",
    "         #\"R18_GRU\",\n",
    "        # \"R18_BiLSTM\",\n",
    "        # \"R18_BiGRU\",\n",
    "        # \"R34_LSTM\",\n",
    "         #\"R34_GRU\",\n",
    "        # \"R34_BiLSTM\",\n",
    "        # \"R34_BiGRU\",\n",
    "        #\"R50_LSTM\",\n",
    "        \"R50_GRU\",\n",
    "        # \"R50_BiLSTM\",\n",
    "        # \"R50_BiGRU\",\n",
    "        # \"R101_LSTM\",\n",
    "        \"R101_GRU\",\n",
    "        # \"R101_BiLSTM\",\n",
    "        #\"R101_BiGRU\",\n",
    "        \"ViViT_small\",\n",
    "        \"ViViT_large\"\n",
    "    ]),\n",
    "    \"optim\": hp.choice(\"optim\",[\n",
    "        # {\n",
    "        #     \"name\":\"SGD\",\n",
    "        #     \"params\": {\n",
    "        #         \"lr\": hp.loguniform(\"lr-1\", np.log(1e-6), np.log(1e-2)),\n",
    "        #         \"momentum\": hp.uniform(\"momentum-1\", 0.1, 0.95),\n",
    "        #         \"weight_decay\": hp.loguniform(\"weight_decay-1\", np.log(1e-7), np.log(1e-2))\n",
    "        #     }\n",
    "        # },\n",
    "        # {\n",
    "        #     \"name\":\"RMSProp\",\n",
    "        #     \"params\": {\n",
    "        #         \"lr\": hp.loguniform(\"lr-2\", np.log(1e-6), np.log(1e-2)),\n",
    "        #         \"momentum\": hp.uniform(\"momentum-2\", 0.1, 0.95),\n",
    "        #         \"weight_decay\": hp.loguniform(\"weight_decay-2\", np.log(1e-7), np.log(1e-2))\n",
    "        #     }\n",
    "        # },\n",
    "        {\n",
    "            \"name\":\"Adam\",\n",
    "            \"params\": {\n",
    "                \"lr\": hp.choice(\"lr-3\", [1e-3, 1e-4]),\n",
    "                \"weight_decay\": hp.choice(\"weight_decay-3\", [3.310305423548208e-05])\n",
    "            }\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\":\"AdamW\",\n",
    "        #     \"params\": {\n",
    "        #         \"lr\": hp.loguniform(\"lr-4\", np.log(1e-6), np.log(1e-4)),\n",
    "        #         \"weight_decay\": hp.loguniform(\"weight_decay-4\", np.log(1e-7), np.log(1e-2))\n",
    "        #     }\n",
    "        # },\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the average train loss over the space\n",
    "trials = Trials()\n",
    "max_evals = 1 if RUN_MODE == \"DEV\" else 192\n",
    "best = fmin(train_model_hyperopt, search_space, algo=tpe.suggest, max_evals=max_evals, trials=trials, verbose=False)\n",
    "print(space_eval(search_space, best))\n",
    "# export log\n",
    "export_hyperopt_log(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:36.958247158Z",
     "start_time": "2023-05-28T10:30:30.711961323Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model with the best parameter\n",
    "best_params = {'epochs': 1, 'model': 'Eff1_BiGRU_pretrained', 'optim': {'name': 'Adam', 'params': {\n",
    "     'lr': 0.001, 'weight_decay': 3.310305423548208e-05}}}\n",
    "#best_params = space_eval(search_space, best) #comment in case of changing parameters\n",
    "best_model, train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(best_params)\n",
    "torch.save(best_model, \"seq_images.pth\")\n",
    "\n",
    "plot_model_stats(type(best_model).__name__, train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n",
    "evaluate_model(best_model, test_loader, verbose=True, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_model = torch.load(model_save_path + \"model.pth\")\n",
    "evaluate_model(best_model, test_loader, verbose=True, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T17:52:29.575693Z",
     "start_time": "2023-05-06T17:52:29.546693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def get_all_conv_layers(model, modules_list=None, conv_layers=[], depth=0, grad_cam=False, feature_map=False):\n",
    "    \"\"\"\n",
    "    Get all the convolutional layers of a given model\n",
    "    \"\"\"\n",
    "    if modules_list is None:\n",
    "        modules_list = list(model.modules())\n",
    "\n",
    "    # get all the conv layers so that the last layer is used for grad cam visualisation\n",
    "    if grad_cam and (not feature_map):\n",
    "        for layer in modules_list:\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                conv_layers.append(layer)\n",
    "            elif isinstance(layer, torch.nn.Sequential):\n",
    "                get_all_conv_layers(model, layer, conv_layers, depth=depth + 1)\n",
    "\n",
    "    # get all inner conv layers for feature map visualisation\n",
    "    elif feature_map and (not grad_cam):\n",
    "        for layer in modules_list:\n",
    "            if isinstance(layer, torch.nn.Conv2d) and depth > 0:\n",
    "                conv_layers.append(layer)\n",
    "            elif isinstance(layer, torch.nn.Sequential) and depth > 0:\n",
    "                get_all_conv_layers(model, layer, conv_layers, depth=depth + 1)\n",
    "\n",
    "    return conv_layers\n",
    "\n",
    "\n",
    "def visualise_feature_maps(feature_map, feature_map_name):\n",
    "    \"\"\"\n",
    "    Visualise the feature maps of a given layer\n",
    "    \"\"\"\n",
    "    feature_map = feature_map.cpu().numpy()\n",
    "\n",
    "    # Get the number of feature maps\n",
    "    num_feature_maps = feature_map.shape[1]\n",
    "\n",
    "    # Calculate the number of rows and columns for the plot\n",
    "    num_cols = 8\n",
    "    num_rows = num_feature_maps // num_cols + int(num_feature_maps % num_cols > 0)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "    plots = []\n",
    "    for i in range(num_feature_maps):\n",
    "        ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.imshow(feature_map[0, i], cmap=\"viridis\")\n",
    "        ax.axis(\"off\")\n",
    "        plots.append(feature_map[0, i])\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(num_feature_maps, num_rows * num_cols):\n",
    "        axes[i // num_cols, i % num_cols].axis(\"off\")\n",
    "\n",
    "    plt.savefig(feature_map_name)\n",
    "    plt.close('all')\n",
    "\n",
    "    # return the figure\n",
    "    return plots\n",
    "\n",
    "\n",
    "def normalize_feature_map(feature_map):\n",
    "    min_val, max_val = np.min(feature_map), np.max(feature_map)\n",
    "    return (feature_map - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def visualise_gradcam(images_numpy, cam, ith_image, seq_idx, i, grad_img_i_folder, max_gradcam_images=3, show_gradcam=True, class_name=\"\"):\n",
    "    \"\"\"\n",
    "    save GradCAMs for the given image\n",
    "    \"\"\"\n",
    "\n",
    "    if show_gradcam and (i < max_gradcam_images):\n",
    "        print(f\"Extracting grad cam for image {i + 1}_{seq_idx}/{max_gradcam_images}\")\n",
    "\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images_numpy)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        grayscale_cam = cam(input_tensor=ith_image, targets=None)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(grayscale_cam)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        visualization = show_cam_on_image(images_numpy, grayscale_cam, use_rgb=True, image_weight=0.8)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(visualization)\n",
    "        plt.gca().set_title(class_name, fontsize=40, pad=20, y=-0.2)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(grad_img_i_folder + f\"image_{seq_idx + 1}_{class_name}.png\")\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "def show_all_feature_maps(target_layers, model, ith_image, i, seq_index, layers_folder):\n",
    "    \"\"\"\n",
    "    Show all the feature maps of all the target layers, of a given model, in a given image\n",
    "    \"\"\"\n",
    "\n",
    "    # get feature maps for each detected target layers\n",
    "    list_of_plots = {}\n",
    "    valid_target_layers = []\n",
    "\n",
    "    # Extract feature maps for each target layer and save them, if they are valid\n",
    "    cnt = 0\n",
    "    for j, layer in enumerate(target_layers):\n",
    "        feature_maps = []\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            feature_maps.append(output.detach())\n",
    "\n",
    "        # register hook to the layer to get the feature maps\n",
    "        layer.register_forward_hook(hook_fn)\n",
    "        model(ith_image)\n",
    "\n",
    "        # if no feature maps were found, skip this layer\n",
    "        if len(feature_maps) == 0:\n",
    "            layer._forward_hooks.clear()\n",
    "            continue\n",
    "\n",
    "        if feature_maps[0].shape[-1] <=1:\n",
    "            layer._forward_hooks.clear()\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracting feature maps for seq {i + 1}, image {seq_index}, from layer {j + 1}/{len(target_layers)}\")\n",
    "\n",
    "        # save feature maps using this function\n",
    "        plots = visualise_feature_maps(feature_maps[0], f\"{layers_folder}layer_{cnt + 1}.png\")\n",
    "        cnt += 1\n",
    "\n",
    "        # update the list of valid target layers and the list of plots\n",
    "        list_of_plots[j] = plots\n",
    "        valid_target_layers.append((layer, j))  # save the layer and its index\n",
    "\n",
    "        layer._forward_hooks.clear()\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "    return list_of_plots, valid_target_layers\n",
    "\n",
    "\n",
    "def create_chart(num_single_chart_layers, num_single_chart_conv_imgs, valid_target_layers, list_of_plots, layers_folder_prev, i):\n",
    "    \"\"\"\n",
    "    Create a chart with num_single_chart_layers layers and num_single_chart_conv_imgs feature maps per layer\n",
    "    \"\"\"\n",
    "\n",
    "    # select the smallest of chosen number of rows in the chart and number of valid target layers\n",
    "    num_single_chart_layers = min(num_single_chart_layers, len(valid_target_layers))\n",
    "\n",
    "    # pick num_single_chart_layers random layers without changing the order\n",
    "    remove_layers_numbers = random.sample(valid_target_layers, max(len(valid_target_layers) - num_single_chart_layers, 0))\n",
    "    valid_target_layers = [layer for layer in valid_target_layers if layer not in remove_layers_numbers]\n",
    "\n",
    "    # put all layers in one image\n",
    "    print(f\"Merging all layers in one image...\")\n",
    "    plt.figure(figsize=(10 * num_single_chart_conv_imgs, 10 * num_single_chart_layers))\n",
    "    cnt = 1\n",
    "\n",
    "    # merge all feature map plots in one image to form a chart\n",
    "    for j, layer in enumerate(valid_target_layers):\n",
    "        layer, layer_index = layer\n",
    "        plots = list_of_plots[layer_index]\n",
    "\n",
    "        # pick num_single_chart_conv_imgs random features per layer without changing the order\n",
    "        if num_single_chart_conv_imgs is not None:\n",
    "            plots_numbers = [random.randint(0, len(plots) - 1) for _ in range(num_single_chart_conv_imgs)]\n",
    "            plots_numbers = sorted(plots_numbers)\n",
    "            plots = [plots[i] for i in plots_numbers]\n",
    "        else:\n",
    "            num_single_chart_conv_imgs = len(plots)\n",
    "\n",
    "        # all plots in the selected layer\n",
    "        for k, plot in enumerate(plots):\n",
    "            plot = normalize_feature_map(plot)\n",
    "\n",
    "            subplot = plt.subplot(len(valid_target_layers), num_single_chart_conv_imgs, cnt)  # (*nrows*, *ncols*, *index*)\n",
    "            cnt += 1\n",
    "\n",
    "            plt.imshow(plot, cmap=\"viridis\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Add \"Row_j\" ylabel to the first subplot of each row\n",
    "            if k == 0:\n",
    "                label_axis = subplot.twinx()\n",
    "                label_axis.set_ylabel(f\"Layer_{layer_index + 1}\", fontsize=40, rotation=0, labelpad=160)\n",
    "                label_axis.yaxis.set_label_position(\"left\")\n",
    "                label_axis.yaxis.tick_left()\n",
    "                label_axis.yaxis.set_ticks([])\n",
    "                label_axis.xaxis.set_ticks([])\n",
    "\n",
    "    # plt.savefig(f\"feature_maps{os.sep}image_{i + 1}{os.sep}Chart.png\")\n",
    "    plt.savefig(layers_folder_prev + f\"Chart.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def get_gradcam_feature_maps(model, test_loader, show_gradcam=False, max_gradcam_images=5, show_feature_map=False, max_feature_map_images=3, max_feature_map_classes=2, num_feature_map_seqs_per_class=2, num_single_chart_layers=None, num_single_chart_conv_imgs=None, class_list=class_list):\n",
    "    \"\"\"\n",
    "    Compute GradCAM and feature maps for a given model and a given test_loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Get all conv layers from the given model and get the last layer to visualize in GradCAM\n",
    "    layers = get_all_conv_layers(model, feature_map=show_feature_map, grad_cam=show_gradcam)\n",
    "    target_layers = layers.copy()\n",
    "    layer = layers[-1]\n",
    "\n",
    "    if num_single_chart_layers is None:\n",
    "        num_single_chart_layers = len(target_layers)\n",
    "\n",
    "    cam = GradCAM(model=model, target_layers=[layer], use_cuda=True)\n",
    "\n",
    "    # create folders if they don't exist\n",
    "    if show_gradcam:\n",
    "        shutil.rmtree(\"gradcams\", ignore_errors=True)\n",
    "        os.makedirs(\"gradcams\")\n",
    "\n",
    "    if show_feature_map:\n",
    "        shutil.rmtree(\"feature_maps\", ignore_errors=True)\n",
    "        os.makedirs(\"feature_maps\")\n",
    "\n",
    "    # computing predictions and confusion matrix\n",
    "    class_seq_pairs = {}\n",
    "    for i, (images, targets) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "        # convert torch target to numpy and get the class name\n",
    "        targets = targets.numpy()\n",
    "        class_name = class_list[targets[0]]\n",
    "        print(\"Class name:\", class_name)\n",
    "        # continue\n",
    "\n",
    "        if show_feature_map:\n",
    "            max_reached_cnt = 0\n",
    "            for class_name_key in class_seq_pairs:\n",
    "                if len(class_seq_pairs[class_name_key]) >= num_feature_map_seqs_per_class:\n",
    "                    max_reached_cnt += 1\n",
    "\n",
    "            # stop execution when the maximum number of classes needed is reached and all classes are filled\n",
    "            if max_reached_cnt >= max_feature_map_classes:\n",
    "                print(f\"Max number of classes reached and filled: {max_feature_map_classes}\")\n",
    "                break\n",
    "\n",
    "            # skip iteration when the maximum number of classes needed is reached, but current classes are not yet filled\n",
    "            if (len(class_seq_pairs) >= max_feature_map_classes) and (class_name not in class_seq_pairs):\n",
    "                # print(f\"Max number of classes reached, seqs are still needed for current classes\")\n",
    "                continue\n",
    "\n",
    "            # assign the class name and sequence number used within it\n",
    "            if class_name not in class_seq_pairs:\n",
    "                class_seq_pairs[class_name] = [i]\n",
    "            else:\n",
    "                # reached max number of sequences needed for this class\n",
    "                if len(class_seq_pairs[class_name]) >= num_feature_map_seqs_per_class:\n",
    "                    print(f\"Max number of sequences reached for class {class_name}: {num_feature_map_seqs_per_class}\")\n",
    "                    continue\n",
    "                class_seq_pairs[class_name].append(i)\n",
    "\n",
    "        # for grad cams and feature maps create a separate folder for each image\n",
    "        if show_gradcam and i < max_gradcam_images:\n",
    "            grad_img_i_folder = f\"gradcams{os.sep}seq_{i + 1}{os.sep}\"\n",
    "            shutil.rmtree(grad_img_i_folder, ignore_errors=True)\n",
    "            os.makedirs(grad_img_i_folder)\n",
    "\n",
    "        print(f\"images shape\", images.shape)\n",
    "\n",
    "        images, targets = images.to(device, dtype=torch.float), torch.Tensor(targets).to(device)\n",
    "        org_images = images[0]  # we assume that batch size is 1, since it is designed to run on test loader\n",
    "\n",
    "        random_seq_idx = random.randint(0, images.shape[2] - 1 - max_feature_map_images)\n",
    "        num_feature_map_image_cnt = 0\n",
    "        for seq_idx in range(images.shape[2]):\n",
    "            if show_feature_map:\n",
    "                # we need 4 random consecutive images from the same sequence\n",
    "                if seq_idx < random_seq_idx:\n",
    "                    continue\n",
    "\n",
    "            # clear memory\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Get the ith image from the sequence\n",
    "            ith_image = org_images.permute(1, 0, 2, 3)  # ([3, 32, 128, 128]) to ([32, 3, 128, 128])\n",
    "            ith_image = ith_image[seq_idx:seq_idx + 1].clone()  # Create a new tensor to avoid modifying the original one\n",
    "\n",
    "            # No need to convert back to tensor since it's already a tensor\n",
    "            ith_image = ith_image.to(device, dtype=torch.float)\n",
    "            outputs = torch.nn.functional.log_softmax(model(ith_image), dim=1)\n",
    "            preds = torch.argsort(outputs, dim=1, descending=True)[:, :3]\n",
    "\n",
    "            # get numpy array from images\n",
    "            images_numpy = ith_image.cpu().numpy()\n",
    "            images_numpy = np.transpose(images_numpy, (0, 2, 3, 1))\n",
    "            images_numpy = np.squeeze(images_numpy)\n",
    "\n",
    "            # show GradCAMs for the max given images\n",
    "            if show_gradcam and (i < max_gradcam_images):\n",
    "                visualise_gradcam(\n",
    "                    images_numpy=images_numpy,\n",
    "                    cam=cam,\n",
    "                    ith_image=ith_image,\n",
    "                    seq_idx=seq_idx,\n",
    "                    i=i,\n",
    "                    max_gradcam_images=max_gradcam_images,\n",
    "                    show_gradcam=show_gradcam,\n",
    "                    grad_img_i_folder=grad_img_i_folder,\n",
    "                    class_name=class_name,\n",
    "                )\n",
    "\n",
    "            # show feature maps for the max given images\n",
    "            if show_feature_map:\n",
    "                # create folder for each image/sequence and delete the previous one\n",
    "                layers_folder = f\"feature_maps{os.sep}class_{class_name}{os.sep}seq_{len(class_seq_pairs[class_name])}{os.sep}image_{seq_idx + 1}{os.sep}layers{os.sep}\"\n",
    "                layers_folder_prev = layers_folder.replace(\"layers\" + os.sep, \"\")\n",
    "                shutil.rmtree(layers_folder_prev, ignore_errors=True)\n",
    "                os.makedirs(layers_folder, exist_ok=True)\n",
    "\n",
    "                # get feature maps for all layers\n",
    "                list_of_plots, valid_target_layers = show_all_feature_maps(\n",
    "                    target_layers=target_layers,\n",
    "                    model=model,\n",
    "                    ith_image=ith_image,\n",
    "                    i=i,\n",
    "                    seq_index=seq_idx + 1,\n",
    "                    layers_folder=layers_folder\n",
    "                )\n",
    "\n",
    "                # create chart for all layers of the current image\n",
    "                create_chart(\n",
    "                    num_single_chart_layers=num_single_chart_layers,\n",
    "                    num_single_chart_conv_imgs=num_single_chart_conv_imgs,\n",
    "                    valid_target_layers=valid_target_layers,\n",
    "                    list_of_plots=list_of_plots,\n",
    "                    layers_folder_prev=layers_folder_prev,\n",
    "                    i=i\n",
    "                )\n",
    "\n",
    "                num_feature_map_image_cnt += 1\n",
    "\n",
    "                print(f\"\\nFeature maps and chart for sequence {i + 1}, image {seq_idx + 1} saved successfully!\\n\\n\")\n",
    "                if num_feature_map_image_cnt >= max_feature_map_images and show_feature_map:\n",
    "                    print(f\"Max number of feature map images reached: {max_feature_map_images}\")\n",
    "                    break\n",
    "\n",
    "        # stop after max images\n",
    "        if ((i >= max_gradcam_images) and show_gradcam):\n",
    "            break\n",
    "\n",
    "    print(\"\\nMaximum selected sequences completed!\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3mY7RGvk_Wwh"
   ],
   "machine_shape": "hm",
   "name": "",
   "provenance": [
    {
     "file_id": "1rqXdW8x2vLeIlkrv_ImgvhlvTFVVvaAg",
     "timestamp": 1665541249454
    }
   ],
   "version": ""
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cedb816d94db2648df1ed8d299ca42a0d191dc990e77464e1581386c54378e51"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00f6db93742340ae99cc7a770d2b5d45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fa67735a8ff40bb895bd93a28dc5d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b01080703a04600963164e9d21d395e",
      "placeholder": "",
      "style": "IPY_MODEL_fd853d84518f403dabe1b9529b39cd80",
      "value": "100%"
     }
    },
    "218c85645a334a5f9501fe5fd1928613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48d014803cb34d1d97a556aa6a1ac39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e970d730079433ea0dbdee23564cb35",
      "placeholder": "",
      "style": "IPY_MODEL_00f6db93742340ae99cc7a770d2b5d45",
      "value": "100%"
     }
    },
    "5e970d730079433ea0dbdee23564cb35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b01080703a04600963164e9d21d395e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82dbe6b8d15344b5a212d8a6d6da8d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9466404a296843098d47360f37c0ba92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98c00a5615c84119af4fe6b633417351": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0fa67735a8ff40bb895bd93a28dc5d80",
       "IPY_MODEL_c25e7a25081149c5a87f5d466a5fef1f",
       "IPY_MODEL_d88e2b9004504cf29da570d87b01af9b"
      ],
      "layout": "IPY_MODEL_9466404a296843098d47360f37c0ba92"
     }
    },
    "9a612f6b808b447e9b22c9ddf9bb8c58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b42a367042b4f41a0131827cc0f1295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a556607a3d8a41c2a54bcdd0545abd66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a78ee61e3f964c13b8b834f9f4dc6869": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7902ea5614649b28bc96388b1e56f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a612f6b808b447e9b22c9ddf9bb8c58",
      "placeholder": "",
      "style": "IPY_MODEL_9b42a367042b4f41a0131827cc0f1295",
      "value": " 127M/127M [00:00&lt;00:00, 229MB/s]"
     }
    },
    "bda2d71f5d7e46babfeec1bcdf6abcbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9374215bd5d45189e506eee10340f82",
      "max": 133546016,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_218c85645a334a5f9501fe5fd1928613",
      "value": 133546016
     }
    },
    "c25e7a25081149c5a87f5d466a5fef1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82dbe6b8d15344b5a212d8a6d6da8d26",
      "max": 46841888,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a556607a3d8a41c2a54bcdd0545abd66",
      "value": 46841888
     }
    },
    "c739bf23bba54ed38fc97470f541c3c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48d014803cb34d1d97a556aa6a1ac39b",
       "IPY_MODEL_bda2d71f5d7e46babfeec1bcdf6abcbd",
       "IPY_MODEL_a7902ea5614649b28bc96388b1e56f27"
      ],
      "layout": "IPY_MODEL_e11aed3e9a7147c083398689c1931c2d"
     }
    },
    "d2d2976f1d4a47b78a68a3518b8db4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d88e2b9004504cf29da570d87b01af9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a78ee61e3f964c13b8b834f9f4dc6869",
      "placeholder": "",
      "style": "IPY_MODEL_d2d2976f1d4a47b78a68a3518b8db4d6",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 153MB/s]"
     }
    },
    "d9374215bd5d45189e506eee10340f82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e11aed3e9a7147c083398689c1931c2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd853d84518f403dabe1b9529b39cd80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
