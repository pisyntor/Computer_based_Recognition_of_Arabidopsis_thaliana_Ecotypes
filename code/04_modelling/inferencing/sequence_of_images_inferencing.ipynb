{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import cv2 as cv\n",
    "\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "extracted_data_path = 'extracted-data-path'\n",
    "model_path = \"your-model-path\"\n",
    "\n",
    "# Image processing parameters\n",
    "RESIZE_SIZE = (128, 128)\n",
    "DAY_TO_USE = 8\n",
    "IMAGES_IN_A_DAY = 4\n",
    "SEQ_LEN = DAY_TO_USE * IMAGES_IN_A_DAY\n",
    "\n",
    "# Device for inference\n",
    "device = \"cuda\"\n",
    "\n",
    "# Class labels dictionary\n",
    "class_dict = {0: 'Col-0', 1: 'Cvi-0', 2: 'Is-1', 3: 'Kz-9', 4: 'Ler-1', 5: 'TOU-I-17', 6: 'Uk-1', 7: 'Zdr-1'}\n",
    "\n",
    "# List of available class outputs\n",
    "available_outputs = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e54bfb41344ff",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _fix_images_paths(images_paths, targets):\n",
    "    images_paths = deepcopy(images_paths)\n",
    "    targets = deepcopy(targets)\n",
    "\n",
    "    while os.path.basename(images_paths[0]).split('_')[1:4] != os.path.basename(images_paths[1]).split('_')[1:4]:\n",
    "        images_paths = images_paths[1:]\n",
    "        targets = targets[1:]\n",
    "    while os.path.basename(images_paths[-1]).split('_')[1:4] != os.path.basename(images_paths[-2]).split('_')[1:4]:\n",
    "        images_paths = images_paths[:-1]\n",
    "        targets = targets[:-1]\n",
    "    return images_paths, targets\n",
    "\n",
    "\n",
    "def prepare_dataset(image_files, targets, fix_len=10, ):\n",
    "    images_paths, targets_cls = _fix_images_paths(image_files, targets)\n",
    "\n",
    "    data_as_dict = {}\n",
    "    for el, _trg in zip(images_paths, targets_cls):\n",
    "        _cls, _rep_name = el.split(os.sep)[-3:-1]\n",
    "        k = (_cls, _rep_name)\n",
    "        if k not in data_as_dict:\n",
    "            data_as_dict[k] = []\n",
    "        data_as_dict[k].append((el, _trg))\n",
    "\n",
    "    # Add more images to the dataset to make it divisible by fix_len\n",
    "    for k in data_as_dict:\n",
    "        while len(data_as_dict[k]) % fix_len != 0:\n",
    "            data_as_dict[k].append(data_as_dict[k][-1])\n",
    "\n",
    "    datas = {}\n",
    "    for k in data_as_dict:\n",
    "        data_type = 'train'\n",
    "        if data_type not in datas:\n",
    "            datas[data_type] = []\n",
    "        files_in_rep = len(data_as_dict[k])\n",
    "        for start_idx in range(0, files_in_rep - fix_len + 1, 2):\n",
    "            seq = data_as_dict[k][start_idx:start_idx + fix_len]\n",
    "            assert len(seq) == fix_len, '...'\n",
    "            datas[data_type].append(seq)\n",
    "\n",
    "    return datas\n",
    "\n",
    "\n",
    "def load_data_from_ds(extracted_data_path, class_dict):\n",
    "\n",
    "    if not extracted_data_path.endswith(os.sep):\n",
    "        extracted_data_path += os.sep\n",
    "\n",
    "    # some stats about the data\n",
    "    image_files = []\n",
    "    targets = []\n",
    "    class_name_img = []\n",
    "\n",
    "    # loading all image_paths (IN A SORTED ORDER, this is really important to avoid any weird exceptions)\n",
    "    for class_name in class_dict.keys():\n",
    "        if class_name.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        if not os.path.isdir(extracted_data_path + class_name):\n",
    "            continue\n",
    "\n",
    "        repetitions_list = os.listdir(extracted_data_path + class_name)\n",
    "        repetitions_list.sort()\n",
    "        for repetition in repetitions_list:\n",
    "            if repetition.startswith('.'):\n",
    "                continue\n",
    "            image_list = os.listdir(extracted_data_path + class_name + os.sep + repetition)\n",
    "            image_list.sort()\n",
    "            image_files.extend(\n",
    "                [extracted_data_path + class_name + os.sep + repetition + os.sep + img for img in image_list]\n",
    "            )\n",
    "            targets.extend([class_dict[class_name]] * len(image_list))\n",
    "            class_name_img.extend([class_name] * len(image_list))\n",
    "\n",
    "    # targets = np.array(targets)\n",
    "\n",
    "    return image_files, class_name_img, class_dict\n",
    "\n",
    "\n",
    "class ClassificationPlantSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, img_size):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(RESIZE_SIZE[0], RESIZE_SIZE[1]),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq, classes = list(zip(*self.data[index]))\n",
    "        assert len(set(classes)) == 1, 'wrong seq clses'\n",
    "        seq_cls = classes[0]\n",
    "\n",
    "        images = []\n",
    "        for im_path in seq:\n",
    "            img = cv.imread(im_path)\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            img = cv.resize(img, self.img_size)\n",
    "            img = self.transform(image=img)['image']\n",
    "            img = img.astype(np.float32) / 255.\n",
    "            img = torch.from_numpy(img.transpose((2, 0, 1)))\n",
    "            images.append(img)\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        images = torch.permute(images, (1, 0, 2, 3))\n",
    "        return images, seq_cls\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def reverse_dict(d):\n",
    "    return dict([(v, k) for k, v in d.items()])\n",
    "\n",
    "\n",
    "def remove_unavailable_outputs(opt_list, available_outputs):\n",
    "    opt_list = deepcopy(opt_list)\n",
    "    for i, val in enumerate(opt_list):\n",
    "        if i not in available_outputs:\n",
    "            opt_list[i] = 0\n",
    "    return opt_list\n",
    "\n",
    "\n",
    "def evaluate_sequence_model(extracted_data_path, model_path, resize_size, device, class_dict, available_outputs, seq_len):\n",
    "    image_files, targets, _ = load_data_from_ds(extracted_data_path, class_dict=reverse_dict(class_dict))\n",
    "    datas = prepare_dataset(image_files, targets, fix_len=max(seq_len, 10))\n",
    "\n",
    "    available_class_names = []\n",
    "    for i in available_outputs:\n",
    "        available_class_names.append(class_dict[i])\n",
    "\n",
    "    infer_dataset = ClassificationPlantSequenceDataset(datas['train'], img_size=resize_size)\n",
    "\n",
    "    dataloader = DataLoader(infer_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    model = torch.jit.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Aggregated values for metrics\n",
    "    TP_all, TN_all, FP_all, FN_all = 0, 0, 0, 0\n",
    "\n",
    "    for i, batch_data in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Inference on seqs\"):\n",
    "        images_tensor, seq_cls_batch = batch_data\n",
    "\n",
    "        # Move tensors to device\n",
    "        images_tensor = images_tensor.to(device)\n",
    "\n",
    "        # Model inference\n",
    "        outputs = model(images_tensor)\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "        for idx, output in enumerate(outputs):\n",
    "            seq_cls = seq_cls_batch[idx]\n",
    "\n",
    "            # Make unused outputs 0\n",
    "            output_list = remove_unavailable_outputs(output.tolist(), available_outputs)\n",
    "\n",
    "            max_val = np.argmax(output_list)\n",
    "\n",
    "            if seq_cls not in available_class_names:\n",
    "                continue\n",
    "\n",
    "            # print(\"seq_cls: \", seq_cls)\n",
    "            # print(\"opt_cls: \", class_dict[max_val], \"\\n\")\n",
    "\n",
    "            if max_val in available_outputs:\n",
    "                if seq_cls == class_dict[max_val]:\n",
    "                    TP_all += 1\n",
    "                else:\n",
    "                    FP_all += 1\n",
    "            else:\n",
    "                if seq_cls == class_dict[max_val]:\n",
    "                    FN_all += 1\n",
    "                else:\n",
    "                    TN_all += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "    # Now, compute the metrics using the aggregated counts\n",
    "    # To prevent division by zero\n",
    "    eps = 1e-7\n",
    "\n",
    "    precision = TP_all / (TP_all + FP_all + eps)\n",
    "    recall = TP_all / (TP_all + FN_all + eps)\n",
    "    specificity = TN_all / (TN_all + FP_all + eps)\n",
    "    fpr = 1 - specificity\n",
    "    fnr = FN_all / (TP_all + FN_all + eps)\n",
    "    fdr = FP_all / (TP_all + FP_all + eps)\n",
    "    for_ = FN_all / (TN_all + FN_all + eps)  # False Omission Rate\n",
    "    misclassification_rate = (FP_all + FN_all) / (TP_all + TN_all + FP_all + FN_all + eps)\n",
    "    mcc = (TP_all * TN_all - FP_all * FN_all) / ((TP_all + FP_all) * (TP_all + FN_all) * (TN_all + FP_all) * (TN_all + FN_all) + eps) ** 0.5\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "    overall_accuracy = TP_all / total\n",
    "\n",
    "    print(\"Overall Accuracy: \", overall_accuracy)\n",
    "\n",
    "    results = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Specificity': specificity,\n",
    "        'False Positive Rate': fpr,\n",
    "        'False Negative Rate': fnr,\n",
    "        'False Discovery Rate': fdr,\n",
    "        'False Omission Rate': for_,\n",
    "        'Misclassification Rate': misclassification_rate,\n",
    "        'MCC': mcc,\n",
    "        'Overall Accuracy': overall_accuracy\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba50fb6da95b606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e684c26c46a828c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform sequence model evaluation\n",
    "results = evaluate_sequence_model(extracted_data_path, model_path, RESIZE_SIZE, device, class_dict, available_outputs, seq_len=SEQ_LEN)\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
