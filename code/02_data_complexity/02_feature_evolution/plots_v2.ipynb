{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc370dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, path, dpi=300):\n",
    "    \"\"\"Save a figure to a file.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(path, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "INPUT_FILE = \"../03_data_complexity/01_extracted_features/internal_datasets/all_features_cropped_truncated.csv\"\n",
    "OUTPUT_FOLDER = \"\"\n",
    "FEATURES = [\"HoG\", \"Rank #unique colors\"]\n",
    "\n",
    "# Starting dates for different datasets\n",
    "STARTING_DATES = {\n",
    "    'accessions_dataset1': datetime.datetime(year=2022, month=5, day=11, hour=0, minute=0, second=0),\n",
    "    'accessions_dataset2': datetime.datetime(year=2022, month=8, day=2, hour=0, minute=0, second=0),\n",
    "}\n",
    "\n",
    "# Special y-axis limits for specific data\n",
    "SPECIAL_Y_LIM_DATA = {\n",
    "    ('HoG', 'accessions_dataset2'): [4.6, 4.85],\n",
    "}\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Convert paths to Path objects\n",
    "data_path = Path(INPUT_FILE)\n",
    "output_folder = Path(OUTPUT_FOLDER)\n",
    "\n",
    "# Remove the output folder if it exists\n",
    "if output_folder.exists():\n",
    "    shutil.rmtree(output_folder)\n",
    "\n",
    "# Read the data from CSV\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract datetime components from filenames\n",
    "filenames = [name.split('-')[0].split('_')[1:-1] for name in data['filename'].values]\n",
    "datetimes = [\n",
    "    datetime.datetime(\n",
    "        int(name[0]), int(name[1]), int(name[2]), 0, 0, 0\n",
    "    ) for name in filenames\n",
    "]\n",
    "data['datetimes'] = datetimes\n",
    "\n",
    "# Sort data by datetimes and reset the index\n",
    "data = data.sort_values(by='datetimes').reset_index(drop=True)\n",
    "\n",
    "# Add a string representation of datetimes\n",
    "data['datetimes_str'] = data['datetimes'].apply(lambda x: str(x))\n",
    "\n",
    "# Iterate over groups of datasets, classes, and replicates\n",
    "for dataset_name, dataset_data in data.groupby('dataset'):\n",
    "    for class_name, class_data in dataset_data.groupby('class'):\n",
    "        for replicate_name, replicate_data in class_data.groupby('replicate'):\n",
    "            \n",
    "            starting_date = replicate_data['datetimes'].min()\n",
    "            \n",
    "            if dataset_name == 'accessions_dataset1':\n",
    "                # Ensure the starting date matches the expected date for dataset1\n",
    "                if starting_date == STARTING_DATES[dataset_name]:\n",
    "                    data.loc[replicate_data.index, 'datetimes_corrected'] = replicate_data['datetimes']\n",
    "                else:\n",
    "                    raise ValueError('Wrong starting date for accessions_dataset1')\n",
    "            \n",
    "            elif dataset_name == 'accessions_dataset2':\n",
    "                # Ensure the starting date matches the expected date for dataset2 or is one week later\n",
    "                if starting_date == STARTING_DATES[dataset_name]:\n",
    "                    data.loc[replicate_data.index, 'datetimes_corrected'] = replicate_data['datetimes']\n",
    "                elif starting_date == STARTING_DATES[dataset_name] + datetime.timedelta(days=7):\n",
    "                    data.loc[replicate_data.index, 'datetimes_corrected'] = replicate_data['datetimes'] - datetime.timedelta(days=7)\n",
    "                else:\n",
    "                    raise ValueError('Wrong starting date for accessions_dataset2')\n",
    "                    \n",
    "# Sort data by corrected datetimes and reset the index\n",
    "data = data.sort_values(by='datetimes_corrected').reset_index(drop=True)\n",
    "\n",
    "# Add a string representation of corrected datetimes\n",
    "data['datetimes_corrected_str'] = data['datetimes_corrected'].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries_pale(dataset_name, ds_data, feature, level=\"class\", transparency=1, special_y_lim=None):\n",
    "    \n",
    "    # Group data by the specified level and datetimes, and calculate the mean\n",
    "    g_data = ds_data[[level, 'datetimes_corrected_str', feature]].groupby([level, 'datetimes_corrected_str']).mean().reset_index()\n",
    "    \n",
    "    # Calculate the mean and standard deviation across groups\n",
    "    gm_data = g_data[['datetimes_corrected_str', feature]].groupby(['datetimes_corrected_str']).mean()\n",
    "    gm_data.rename(columns={feature: f'Orig. {feature} Mean'}, inplace=True)\n",
    "    \n",
    "    gs_data = g_data[['datetimes_corrected_str', feature]].groupby(['datetimes_corrected_str']).std()\n",
    "    gs_data.rename(columns={feature: f'Orig. {feature} Std'}, inplace=True)\n",
    "    \n",
    "    # Join the mean and standard deviation data\n",
    "    j_data = gm_data.join(gs_data)\n",
    "    n_std = 2\n",
    "    j_data[f'Orig. {feature} Mean - {n_std} x Std'] = j_data[f'Orig. {feature} Mean'] - n_std * j_data[f'Orig. {feature} Std']\n",
    "    j_data[f'Orig. {feature} Mean + {n_std} x Std'] = j_data[f'Orig. {feature} Mean'] + n_std * j_data[f'Orig. {feature} Std']\n",
    "    j_data.reset_index(inplace=True)\n",
    "    \n",
    "    # Apply Gaussian filter for smoothing\n",
    "    sigma = j_data.shape[0] / 10\n",
    "    j_data[f'{feature} Mean'] = gaussian_filter1d(j_data[f'Orig. {feature} Mean'], sigma)\n",
    "    j_data[f'{feature} Mean - {n_std} x Std'] = gaussian_filter1d(j_data[f'Orig. {feature} Mean - {n_std} x Std'], sigma)\n",
    "    j_data[f'{feature} Mean + {n_std} x Std'] = gaussian_filter1d(j_data[f'Orig. {feature} Mean + {n_std} x Std'], sigma)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "    \n",
    "    # Set y-axis limits if specified\n",
    "    if (feature, dataset_name) in special_y_lim:\n",
    "        plt.ylim(special_y_lim[(feature, dataset_name)])\n",
    "\n",
    "    # Plot each group's data with transparency\n",
    "    hue_order = sorted(ds_data[level].unique())\n",
    "    sns.lineplot(data=ds_data, x='datetimes_corrected_str', y=feature, hue=level, hue_order=hue_order, ax=ax, errorbar=None)\n",
    "    for line in ax.lines:\n",
    "        line.set_alpha(transparency)\n",
    "\n",
    "    # Plot the mean and standard deviation lines\n",
    "    sns.lineplot(data=j_data, x='datetimes_corrected_str', y=f'{feature} Mean',\n",
    "                 color='red', dashes=[10, 10], linewidth=5, ax=ax, errorbar=None)\n",
    "    sns.lineplot(data=j_data, x='datetimes_corrected_str', y=f'{feature} Mean - {n_std} x Std',\n",
    "                 color='black', linewidth=5, ax=ax, errorbar=None)\n",
    "    sns.lineplot(data=j_data, x='datetimes_corrected_str', y=f'{feature} Mean + {n_std} x Std',\n",
    "                 color='black', linewidth=5, ax=ax, errorbar=None)\n",
    "    \n",
    "    # Format x-axis labels\n",
    "    xticklabels = ds_data['datetimes_corrected_str'].unique()\n",
    "    xticklabels = [x.split(' ')[0] for x in xticklabels]\n",
    "    ax.xaxis.set_ticks(range(len(xticklabels)))\n",
    "    ax.set_xticklabels(xticklabels, rotation=45)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Date', fontsize=14)\n",
    "    ax.set_ylabel(feature, fontsize=14)\n",
    "    ax.set_title(f'{feature} over time', fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Add grid and legend\n",
    "    ax.grid()\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each dataset in the data\n",
    "for dataset_name in tqdm(data['dataset'].unique(), desc=\"Processing datasets...\"):\n",
    "    # Filter dataset-specific data\n",
    "    dataset_data = data[data['dataset'] == dataset_name]\n",
    "\n",
    "    # Loop through each feature for the dataset\n",
    "    for feature in tqdm(FEATURES, desc=\"Processing class features...\", leave=False):\n",
    "        # Generate the plot for the current dataset and feature\n",
    "        fig = plot_timeseries_pale(dataset_name, dataset_data, feature, level=\"class\", transparency=0.3, special_y_lim=SPECIAL_Y_LIM_DATA)\n",
    "        \n",
    "        # Create a filename for the saved image\n",
    "        feature_filename = '_'.join([x.lower() for x in feature.split(' ')])\n",
    "        save_path = output_folder / dataset_name / f\"{feature_filename}.png\"\n",
    "        \n",
    "        # Save the plot and close the figure\n",
    "        save_figure(fig, save_path)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:plants_p1] *",
   "language": "python",
   "name": "conda-env-plants_p1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
